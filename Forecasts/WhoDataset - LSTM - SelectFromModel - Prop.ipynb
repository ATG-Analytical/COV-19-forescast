{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.layouts import gridplot\n",
    "import numpy as np\n",
    "import math\n",
    "import urllib\n",
    "from numpy import inf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Descargas/full_data.csv', <http.client.HTTPMessage at 0x7fdf04fb6e48>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = 'https://covid.ourworldindata.org/data/ecdc/full_data.csv'\n",
    "\n",
    "urllib.request.urlretrieve(link, 'Descargas/full_data.csv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('Descargas/full_data.csv').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpainCases = Data[Data['location']=='Spain'].drop('location', axis=1)\n",
    "ItalyCases = Data[Data['location']=='Italy'].drop('location', axis=1)\n",
    "ChinaCases = Data[Data['location']=='China'].drop('location', axis=1)\n",
    "GermanyCases = Data[Data['location']=='Germany'].drop('location', axis=1)\n",
    "FranceCases = Data[Data['location']=='France'].drop('location', axis=1)\n",
    "UKCases = Data[Data['location']=='United Kingdom'].drop('location', axis=1)\n",
    "KoreaCases = Data[Data['location']=='South Korea'].drop('location', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "SpainMortality = SpainCases['total_deaths'][-1:]/SpainCases['total_cases'][-1:]*100\n",
    "ItalyMortality = ItalyCases['total_deaths'][-1:]/ItalyCases['total_cases'][-1:]*100\n",
    "ChinaMortality = ChinaCases['total_deaths'][-1:]/ChinaCases['total_cases'][-1:]*100\n",
    "GermanyMortality = GermanyCases['total_deaths'][-1:]/GermanyCases['total_cases'][-1:]*100\n",
    "FranceMortality = FranceCases['total_deaths'][-1:]/FranceCases['total_cases'][-1:]*100\n",
    "UKMortality = UKCases['total_deaths'][-1:]/UKCases['total_cases'][-1:]*100\n",
    "KoreaMortality = KoreaCases['total_deaths'][-1:]/KoreaCases['total_cases'][-1:]*100\n",
    "Mortality = np.hstack([SpainMortality, ItalyMortality, ChinaMortality, GermanyMortality, FranceMortality, UKMortality, KoreaMortality])  \n",
    "Names = np.hstack(['Spain', 'Italy', 'China', 'Germany', 'France', 'UK', 'Korea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mortality rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>9.074004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>12.076326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>4.033226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>1.186040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>7.618645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>8.663029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korea</th>\n",
       "      <td>1.729278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Mortality rate\n",
       "Spain          9.074004\n",
       "Italy         12.076326\n",
       "China          4.033226\n",
       "Germany        1.186040\n",
       "France         7.618645\n",
       "UK             8.663029\n",
       "Korea          1.729278"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=Mortality, index=Names, columns=['Mortality rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables: \n",
    "    * Cancelación de clases\n",
    "    * Presencia de eventos particularmente públicos masivos\n",
    "    * Estado de alarma\n",
    "    * Transporte público abierto\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Medical_Quality'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Medical_Quality'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ab4b825abf76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mProp_1Medical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChina_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mProp_1Medical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProp_1Medical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mChina_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Medical_Quality'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChina_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mChina_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Eventos_masivos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Eventos_masivos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Estado_de_Alarma'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Transporte_publico'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ejercicio_en_publico'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tiendas_abiertas_al_publico'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Medical_Quality'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Medical_Quality'"
     ]
    }
   ],
   "source": [
    "China = ChinaCases[ChinaCases.index>(ChinaCases[ChinaCases['total_deaths']>5].index-10)[0]]\n",
    "\n",
    "\n",
    "R0_1_China = [2.5]\n",
    "R0_2_China = [2]\n",
    "R0_3_China = [1.5]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "China_1 = China\n",
    "\n",
    "China_1['dia'] = np.arange(0, len(China))\n",
    "China_1['Clases'] = R0_1_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_1['Eventos_masivos'] = R0_1_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_1['Estado_de_Alarma'] = R0_1_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_1['Transporte_publico'] = R0_1_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_1['Ejercicio_en_publico'] = R0_1_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_1['Tiendas_abiertas_al_publico'] = R0_1_China*np.hstack([np.full((len(China[China['date']<='2020-02-13'])), 1), np.full((len(China[China['date']>'2020-02-13'])), 0.25)]) \n",
    "China_1 = China_1.fillna(0)\n",
    "\n",
    "China_2 = China\n",
    "\n",
    "China_2['dia'] = np.arange(0, len(China))\n",
    "China_2['Clases'] = R0_2_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_2['Eventos_masivos'] = R0_2_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_2['Estado_de_Alarma'] = R0_2_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_2['Transporte_publico'] = R0_2_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_2['Ejercicio_en_publico'] = R0_2_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_2['Tiendas_abiertas_al_publico'] = R0_2_China*np.hstack([np.full((len(China[China['date']<='2020-02-13'])), 1), np.full((len(China[China['date']>'2020-02-13'])), 0.25)])\n",
    "China_2['Medical_Quality'] = R0_2_China*np.hstack([np.full((len(China)), 0.25)])\n",
    "China_2 = China_2.fillna(0)\n",
    "\n",
    "China_3 = China\n",
    "\n",
    "\n",
    "China_3['dia'] = np.arange(0, len(China))\n",
    "China_3['Clases'] = R0_3_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_3['Eventos_masivos'] = R0_3_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_3['Estado_de_Alarma'] = R0_3_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_3['Transporte_publico'] = R0_3_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_3['Ejercicio_en_publico'] = R0_3_China*np.hstack([np.full((len(China[China['date']<='2020-01-23'])), 1), np.full((len(China[China['date']>'2020-01-23'])), 0.25)])\n",
    "China_3['Tiendas_abiertas_al_publico'] = R0_3_China*np.hstack([np.full((len(China[China['date']<='2020-02-13'])), 1), np.full((len(China[China['date']>'2020-02-13'])), 0.25)]) \n",
    "China_3['Medical_Quality'] = R0_3_China*np.hstack([np.full((len(China)), 0.25)])\n",
    "China_3 = China_3.fillna(0)\n",
    "\n",
    "\n",
    "Prop_1Clases = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_1Clases.append(Prop_1Clases[-1]*China_1['Clases'][China_1.index[0]+i])\n",
    "Prop_1Eventos = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_1Eventos.append(Prop_1Eventos[-1]*China_1['Eventos_masivos'][China_1.index[0]+i] )\n",
    "Prop_1Alarma = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_1Alarma.append(Prop_1Alarma[-1]*China_1['Estado_de_Alarma'][China_1.index[0]+i])\n",
    "Prop_1Transprte = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_1Transprte.append(Prop_1Transprte[-1]*China_1['Transporte_publico'][China_1.index[0]+i])\n",
    "Prop_1Ejercicio = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_1Ejercicio.append(Prop_1Ejercicio[-1]*China_1['Ejercicio_en_publico'][China_1.index[0]+i])\n",
    "Prop_1Tiendas = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_1Tiendas.append(Prop_1Tiendas[-1]*China_1['Tiendas_abiertas_al_publico'][China_1.index[0]+i])\n",
    "Prop_1Medical = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_1Medical.append(Prop_1Medical[-1]*China_1['Medical_Quality'][China_1.index[0]+i])    \n",
    "    \n",
    "China_1.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "China_1['Clases'] = Prop_1Clases\n",
    "China_1['Eventos_masivos'] = Prop_1Eventos\n",
    "China_1['Estado_de_Alarma'] = Prop_1Alarma\n",
    "China_1['Transporte_publico'] = Prop_1Transprte\n",
    "China_1['Ejercicio_en_publico'] = Prop_1Ejercicio\n",
    "China_1['Tiendas_abiertas_al_publico'] = Prop_1Tiendas\n",
    "China_1['Medical_Quality'] = Prop_1Medical\n",
    "\n",
    "    \n",
    "    \n",
    "Prop_2Clases = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_2Clases.append(Prop_2Clases[-1]*China_2['Clases'][China_1.index[0]+i])\n",
    "Prop_2Eventos = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_2Eventos.append(Prop_2Eventos[-1]*China_2['Eventos_masivos'][China_1.index[0]+i] )\n",
    "Prop_2Alarma = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_2Alarma.append(Prop_2Alarma[-1]*China_2['Estado_de_Alarma'][China_1.index[0]+i])\n",
    "Prop_2Transprte = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_2Transprte.append(Prop_2Transprte[-1]*China_2['Transporte_publico'][China_1.index[0]+i])\n",
    "Prop_2Ejercicio = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_2Ejercicio.append(Prop_2Ejercicio[-1]*China_2['Ejercicio_en_publico'][China_1.index[0]+i])\n",
    "Prop_2Tiendas = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_2Tiendas.append(Prop_2Tiendas[-1]*China_2['Tiendas_abiertas_al_publico'][China_1.index[0]+i])\n",
    "Prop_2Medical = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_2Medical.append(Prop_2Medical[-1]*China_2['Medical_Quality'][China_1.index[0]+i])    \n",
    "    \n",
    "China_2.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "China_2['Clases'] = Prop_2Clases\n",
    "China_2['Eventos_masivos'] = Prop_2Eventos\n",
    "China_2['Estado_de_Alarma'] = Prop_2Alarma\n",
    "China_2['Transporte_publico'] = Prop_2Transprte\n",
    "China_2['Ejercicio_en_publico'] = Prop_2Ejercicio\n",
    "China_2['Tiendas_abiertas_al_publico'] = Prop_2Tiendas\n",
    "China_2['Medical_Quality'] = Prop_2Medical\n",
    "\n",
    "    \n",
    "    \n",
    "Prop_3Clases = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_3Clases.append(Prop_3Clases[-1]*China_3['Clases'][China_1.index[0]+i])\n",
    "Prop_3Eventos = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_3Eventos.append(Prop_3Eventos[-1]*China_3['Eventos_masivos'][China_1.index[0]+i])\n",
    "Prop_3Alarma = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_3Alarma.append(Prop_3Alarma[-1]*China_3['Estado_de_Alarma'][China_1.index[0]+i])\n",
    "Prop_3Transprte = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_3Transprte.append(Prop_3Transprte[-1]*China_3['Transporte_publico'][China_1.index[0]+i])\n",
    "Prop_3Ejercicio = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_3Ejercicio.append(Prop_3Ejercicio[-1]*China_3['Ejercicio_en_publico'][China_1.index[0]+i])\n",
    "Prop_3Tiendas = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_3Tiendas.append(Prop_3Tiendas[-1]*China_3['Tiendas_abiertas_al_publico'][China_1.index[0]+i])\n",
    "Prop_3Medical = [10]\n",
    "for i in np.arange(0, len(China_3)-1):\n",
    "    Prop_3Medical.append(Prop_3Medical[-1]*China_3['Medical_Quality'][China_1.index[0]+i])    \n",
    "    \n",
    "China_3.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "China_3['Clases'] = Prop_3Clases\n",
    "China_3['Eventos_masivos'] = Prop_3Eventos\n",
    "China_3['Estado_de_Alarma'] = Prop_3Alarma\n",
    "China_3['Transporte_publico'] = Prop_3Transprte\n",
    "China_3['Ejercicio_en_publico'] = Prop_3Ejercicio\n",
    "China_3['Tiendas_abiertas_al_publico'] = Prop_3Tiendas\n",
    "China_3['Medical_Quality'] = Prop_3Medical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirando desde que la propagación es comunitaria (>100 casos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancelación de clases, tomando Madrid, que es el principal foco, ocurrido el 9 de marzo, aunque a nivel nacional esas medidas se siguieron de forma muy seguida por el resto de comunidades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En España se permitieron eventos masivos hasta el estado de alarma, al contrario que en otros países donde sí que hubo medidas intermedias, por loque en este caso son factores contrarios sin más"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El transporte público sigue aún hoy día abierto, aunque con restricciones y mayores medidas de seguridad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "Spain = SpainCases[SpainCases.index>(SpainCases[SpainCases['total_deaths']>5].index-10)[0]]\n",
    "\n",
    "\n",
    "\n",
    "R0_1_Spain = [2.5]\n",
    "R0_2_Spain = [2]\n",
    "R0_3_Spain = [1.5]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Spain_1 = Spain\n",
    "\n",
    "Spain_1['dia'] = np.arange(0, len(Spain))\n",
    "Spain_1['Clases'] = R0_1_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-09'])), 1), np.full((len(Spain[Spain['date']>'2020-03-09'])), 0.25)]) \n",
    "Spain_1['Eventos_masivos'] = R0_1_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_1['Estado_de_Alarma'] = R0_1_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_1['Transporte_publico'] = R0_1_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.5)]) \n",
    "Spain_1['Ejercicio_en_publico'] = R0_1_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_1['Tiendas_abiertas_al_publico'] = R0_1_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_1['Medical_Quality'] = R0_1_Spain*np.hstack([np.full((len(Spain)), 0.25)])\n",
    "Spain_1 = Spain_1.fillna(0)\n",
    "\n",
    "Spain_2 = Spain\n",
    "\n",
    "Spain_2['dia'] = np.arange(0, len(Spain))\n",
    "Spain_2['Clases'] = R0_2_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-09'])), 1), np.full((len(Spain[Spain['date']>'2020-03-09'])), 0.25)]) \n",
    "Spain_2['Eventos_masivos'] = R0_2_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_2['Estado_de_Alarma'] = R0_2_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_2['Transporte_publico'] = R0_2_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.5)]) \n",
    "Spain_2['Ejercicio_en_publico'] = R0_2_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_2['Tiendas_abiertas_al_publico'] = R0_2_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_2['Medical_Quality'] = R0_2_Spain*np.hstack([np.full((len(Spain)), 0.25)])\n",
    "Spain_2 = Spain_2.fillna(0)\n",
    "\n",
    "Spain_3 = Spain\n",
    "\n",
    "\n",
    "Spain_3['dia'] = np.arange(0, len(Spain_3))\n",
    "Spain_3['Clases'] = R0_3_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-09'])), 1), np.full((len(Spain[Spain['date']>'2020-03-09'])), 0.25)]) \n",
    "Spain_3['Eventos_masivos'] = R0_3_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_3['Estado_de_Alarma'] = R0_3_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_3['Transporte_publico'] = R0_3_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.5)]) \n",
    "Spain_3['Ejercicio_en_publico'] = R0_3_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_3['Tiendas_abiertas_al_publico'] = R0_3_Spain*np.hstack([np.full((len(Spain[Spain['date']<='2020-03-14'])), 1), np.full((len(Spain[Spain['date']>'2020-03-14'])), 0.25)]) \n",
    "Spain_3['Medical_Quality'] = R0_3_Spain*np.hstack([np.full((len(Spain)), 0.25)])\n",
    "Spain_3 = Spain_3.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "Prop_1Clases = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_1Clases.append(Prop_1Clases[-1]*Spain_1['Clases'][Spain_1.index[0]+i])\n",
    "Prop_1Eventos = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_1Eventos.append(Prop_1Eventos[-1]*Spain_1['Eventos_masivos'][Spain_1.index[0]+i] )\n",
    "Prop_1Alarma = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_1Alarma.append(Prop_1Alarma[-1]*Spain_1['Estado_de_Alarma'][Spain_1.index[0]+i])\n",
    "Prop_1Transprte = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_1Transprte.append(Prop_1Transprte[-1]*Spain_1['Transporte_publico'][Spain_1.index[0]+i])\n",
    "Prop_1Ejercicio = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_1Ejercicio.append(Prop_1Ejercicio[-1]*Spain_1['Ejercicio_en_publico'][Spain_1.index[0]+i])\n",
    "Prop_1Tiendas = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_1Tiendas.append(Prop_1Tiendas[-1]*Spain_1['Tiendas_abiertas_al_publico'][Spain_1.index[0]+i])\n",
    "Prop_1Medical = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_1Medical.append(Prop_1Medical[-1]*Spain_1['Medical_Quality'][Spain_1.index[0]+i])    \n",
    "    \n",
    "Spain_1.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "Spain_1['Clases'] = Prop_1Clases\n",
    "Spain_1['Eventos_masivos'] = Prop_1Eventos\n",
    "Spain_1['Estado_de_Alarma'] = Prop_1Alarma\n",
    "Spain_1['Transporte_publico'] = Prop_1Transprte\n",
    "Spain_1['Ejercicio_en_publico'] = Prop_1Ejercicio\n",
    "Spain_1['Tiendas_abiertas_al_publico'] = Prop_1Tiendas\n",
    "Spain_1['Medical_Quality'] = Prop_1Medical\n",
    "\n",
    "    \n",
    "    \n",
    "Prop_2Clases = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_2Clases.append(Prop_2Clases[-1]*Spain_2['Clases'][Spain_1.index[0]+i])\n",
    "Prop_2Eventos = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_2Eventos.append(Prop_2Eventos[-1]*Spain_2['Eventos_masivos'][Spain_1.index[0]+i] )\n",
    "Prop_2Alarma = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_2Alarma.append(Prop_2Alarma[-1]*Spain_2['Estado_de_Alarma'][Spain_1.index[0]+i])\n",
    "Prop_2Transprte = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_2Transprte.append(Prop_2Transprte[-1]*Spain_2['Transporte_publico'][Spain_1.index[0]+i])\n",
    "Prop_2Ejercicio = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_2Ejercicio.append(Prop_2Ejercicio[-1]*Spain_2['Ejercicio_en_publico'][Spain_1.index[0]+i])\n",
    "Prop_2Tiendas = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_2Tiendas.append(Prop_2Tiendas[-1]*Spain_2['Tiendas_abiertas_al_publico'][Spain_1.index[0]+i])\n",
    "Prop_2Medical = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_2Medical.append(Prop_2Medical[-1]*Spain_2['Medical_Quality'][Spain_1.index[0]+i])    \n",
    "    \n",
    "Spain_2.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "Spain_2['Clases'] = Prop_2Clases\n",
    "Spain_2['Eventos_masivos'] = Prop_2Eventos\n",
    "Spain_2['Estado_de_Alarma'] = Prop_2Alarma\n",
    "Spain_2['Transporte_publico'] = Prop_2Transprte\n",
    "Spain_2['Ejercicio_en_publico'] = Prop_2Ejercicio\n",
    "Spain_2['Tiendas_abiertas_al_publico'] = Prop_2Tiendas\n",
    "Spain_2['Medical_Quality'] = Prop_2Medical\n",
    "\n",
    "    \n",
    "    \n",
    "Prop_3Clases = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_3Clases.append(Prop_3Clases[-1]*Spain_3['Clases'][Spain_1.index[0]+i])\n",
    "Prop_3Eventos = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_3Eventos.append(Prop_3Eventos[-1]*Spain_3['Eventos_masivos'][Spain_1.index[0]+i])\n",
    "Prop_3Alarma = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_3Alarma.append(Prop_3Alarma[-1]*Spain_3['Estado_de_Alarma'][Spain_1.index[0]+i])\n",
    "Prop_3Transprte = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_3Transprte.append(Prop_3Transprte[-1]*Spain_3['Transporte_publico'][Spain_1.index[0]+i])\n",
    "Prop_3Ejercicio = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_3Ejercicio.append(Prop_3Ejercicio[-1]*Spain_3['Ejercicio_en_publico'][Spain_1.index[0]+i])\n",
    "Prop_3Tiendas = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_3Tiendas.append(Prop_3Tiendas[-1]*Spain_3['Tiendas_abiertas_al_publico'][Spain_1.index[0]+i])\n",
    "Prop_3Medical = [10]\n",
    "for i in np.arange(0, len(Spain_3)-1):\n",
    "    Prop_3Medical.append(Prop_3Medical[-1]*Spain_3['Medical_Quality'][Spain_1.index[0]+i])    \n",
    "    \n",
    "Spain_3.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "Spain_3['Clases'] = Prop_3Clases\n",
    "Spain_3['Eventos_masivos'] = Prop_3Eventos\n",
    "Spain_3['Estado_de_Alarma'] = Prop_3Alarma\n",
    "Spain_3['Transporte_publico'] = Prop_3Transprte\n",
    "Spain_3['Ejercicio_en_publico'] = Prop_3Ejercicio\n",
    "Spain_3['Tiendas_abiertas_al_publico'] = Prop_3Tiendas\n",
    "Spain_3['Medical_Quality'] = Prop_3Medical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el Norte de Italia las clases se suspendieron regionalmente el 22 de Febrero, junto a  la mayoría de eventos masivos (partidos de futbol, carnaval de Venecia). En cualquier caso, se suspendieron únicamente de manera regional, por lo que le asignamos un 0.75 en vez de un 0 tras esta cancelación. Las tiendas seguían abiertas aunque con horario restringido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tasa de contagio (R0) del virus se asume que es 2.5, por lo que podemos hacer una función geométrica a partir del número de casos totales. aunque el número de casos totales sea mayor que el oficial y el número oficial esté mayormente en cuarentena, vamos a hacer diferentes estimaciones de la tasa de contagios a partir de esa suposición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "Italy = ItalyCases[ItalyCases.index>(ItalyCases[ItalyCases['total_deaths']>5].index-10)[0]]\n",
    "Italy['dia'] = np.arange(0, len(Italy))\n",
    "\n",
    "\n",
    "\n",
    "R0_1_Italy = [2.5]\n",
    "R0_2_Italy = [2]\n",
    "R0_3_Italy = [1.5]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "Clases = np.hstack([np.full((len(Italy[Italy['date']<='2020-02-22'])), 1), np.full((len(Italy[(Italy['date']>'2020-02-22') & (Italy['date']<='2020-03-04')])), 0.5)])\n",
    "Italy_1 = Italy\n",
    "Italy_1['dia'] = np.arange(0, len(Italy_1))\n",
    "Italy_1['Clases'] = R0_1_Italy*np.hstack([Clases, np.full((len(Italy[Italy['date']>'2020-03-04'])), 0.25)])\n",
    "Italy_1['Eventos_masivos'] = R0_1_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-02-23'])), 1), np.full((len(Italy[Italy['date']>'2020-02-23'])), 0.25)]) \n",
    "Alarma = np.hstack([np.full((len(Italy[Italy['date']<='2020-02-22'])), 1), np.full((len(Italy[(Italy['date']>'2020-02-22') & (Italy['date']<='2020-03-20')])), 0.5)])\n",
    "Italy_1['Estado_de_Alarma'] = R0_1_Italy*np.hstack([Alarma, np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Italy_1['Transporte_publico'] = R0_1_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-03-20'])), 0.75), np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Italy_1['Ejercicio_en_publico'] = R0_1_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-03-20'])), 0.75), np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Tiendas = np.hstack([np.full((len(Italy[Italy['date']<='2020-03-08'])), 0.75), np.full((len(Italy[(Italy['date']>'2020-03-08') & (Italy['date']<='2020-03-11')])), 0.5)])                   \n",
    "Italy_1['Tiendas_abiertas_al_publico'] = R0_1_Italy*np.hstack([Tiendas, np.full((len(Italy[Italy['date']>'2020-03-11'])), 0.25)]) \n",
    "Italy_1['Medical_Quality'] = R0_1_Italy*np.hstack([np.full((len(Italy)), 0.25)])\n",
    "Italy_1 = Italy_1.fillna(0)\n",
    "\n",
    "\n",
    "Clases = np.hstack([np.full((len(Italy[Italy['date']<='2020-02-22'])), 1), np.full((len(Italy[(Italy['date']>'2020-02-22') & (Italy['date']<='2020-03-04')])), 0.5)])\n",
    "Italy_2 = Italy\n",
    "Italy_2['dia'] = np.arange(0, len(Italy_2))\n",
    "Italy_2['Clases'] = R0_2_Italy*np.hstack([Clases, np.full((len(Italy[Italy['date']>'2020-03-04'])), 0.25)])\n",
    "Italy_2['Eventos_masivos'] = R0_2_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-02-23'])), 1), np.full((len(Italy[Italy['date']>'2020-02-23'])), 0.25)]) \n",
    "Alarma = np.hstack([np.full((len(Italy[Italy['date']<='2020-02-22'])), 1), np.full((len(Italy[(Italy['date']>'2020-02-22') & (Italy['date']<='2020-03-20')])), 0.5)])\n",
    "Italy_2['Estado_de_Alarma'] = R0_2_Italy*np.hstack([Alarma, np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Italy_2['Transporte_publico'] = R0_2_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-03-20'])), 0.75), np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Italy_2['Ejercicio_en_publico'] = R0_2_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-03-20'])), 0.75), np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Tiendas = np.hstack([np.full((len(Italy[Italy['date']<='2020-03-08'])), 0.75), np.full((len(Italy[(Italy['date']>'2020-03-08') & (Italy['date']<='2020-03-11')])), 0.5)])                   \n",
    "Italy_2['Tiendas_abiertas_al_publico'] = R0_2_Italy*np.hstack([Tiendas, np.full((len(Italy[Italy['date']>'2020-03-11'])), 0.25)]) \n",
    "Italy_2['Medical_Quality'] = R0_2_Italy*np.hstack([np.full((len(Italy)), 0.25)])\n",
    "Italy_2 = Italy_2.fillna(0)\n",
    "\n",
    "Clases = np.hstack([np.full((len(Italy[Italy['date']<='2020-02-22'])), 1), np.full((len(Italy[(Italy['date']>'2020-02-22') & (Italy['date']<='2020-03-04')])), 0.5)])\n",
    "Italy_3 = Italy\n",
    "Italy_3['dia'] = np.arange(0, len(Italy_3))\n",
    "Italy_3['Clases'] = R0_3_Italy*np.hstack([Clases, np.full((len(Italy[Italy['date']>'2020-03-04'])), 0.25)])\n",
    "Italy_3['Eventos_masivos'] = R0_3_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-02-23'])), 1), np.full((len(Italy[Italy['date']>'2020-02-23'])), 0.25)]) \n",
    "Alarma = np.hstack([np.full((len(Italy[Italy['date']<='2020-02-22'])), 1), np.full((len(Italy[(Italy['date']>'2020-02-22') & (Italy['date']<='2020-03-20')])), 0.5)])\n",
    "Italy_3['Estado_de_Alarma'] = R0_3_Italy*np.hstack([Alarma, np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Italy_3['Transporte_publico'] = R0_3_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-03-20'])), 0.75), np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Italy_3['Ejercicio_en_publico'] = R0_3_Italy*np.hstack([np.full((len(Italy[Italy['date']<='2020-03-20'])), 0.75), np.full((len(Italy[Italy['date']>'2020-03-20'])), 0.25)]) \n",
    "Tiendas = np.hstack([np.full((len(Italy[Italy['date']<='2020-03-08'])), 0.75), np.full((len(Italy[(Italy['date']>'2020-03-08') & (Italy['date']<='2020-03-11')])), 0.5)])                   \n",
    "Italy_3['Tiendas_abiertas_al_publico'] = R0_3_Italy*np.hstack([Tiendas, np.full((len(Italy[Italy['date']>'2020-03-11'])), 0.25)]) \n",
    "Italy_3['Medical_Quality'] = R0_3_Italy*np.hstack([np.full((len(Italy)), 0.25)])\n",
    "Italy_3 = Italy_3.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "Prop_1Clases = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_1Clases.append(Prop_1Clases[-1]*Italy_1['Clases'][Italy_1.index[0]+i])\n",
    "Prop_1Eventos = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_1Eventos.append(Prop_1Eventos[-1]*Italy_1['Eventos_masivos'][Italy_1.index[0]+i] )\n",
    "Prop_1Alarma = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_1Alarma.append(Prop_1Alarma[-1]*Italy_1['Estado_de_Alarma'][Italy_1.index[0]+i])\n",
    "Prop_1Transprte = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_1Transprte.append(Prop_1Transprte[-1]*Italy_1['Transporte_publico'][Italy_1.index[0]+i])\n",
    "Prop_1Ejercicio = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_1Ejercicio.append(Prop_1Ejercicio[-1]*Italy_1['Ejercicio_en_publico'][Italy_1.index[0]+i])\n",
    "Prop_1Tiendas = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_1Tiendas.append(Prop_1Tiendas[-1]*Italy_1['Tiendas_abiertas_al_publico'][Italy_1.index[0]+i])\n",
    "Prop_1Medical = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_1Medical.append(Prop_1Medical[-1]*Italy_1['Medical_Quality'][Italy_1.index[0]+i])    \n",
    "    \n",
    "Italy_1.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "Italy_1['Clases'] = Prop_1Clases\n",
    "Italy_1['Eventos_masivos'] = Prop_1Eventos\n",
    "Italy_1['Estado_de_Alarma'] = Prop_1Alarma\n",
    "Italy_1['Transporte_publico'] = Prop_1Transprte\n",
    "Italy_1['Ejercicio_en_publico'] = Prop_1Ejercicio\n",
    "Italy_1['Tiendas_abiertas_al_publico'] = Prop_1Tiendas\n",
    "Italy_1['Medical_Quality'] = Prop_1Medical\n",
    "\n",
    "    \n",
    "    \n",
    "Prop_2Clases = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_2Clases.append(Prop_2Clases[-1]*Italy_2['Clases'][Italy_1.index[0]+i])\n",
    "Prop_2Eventos = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_2Eventos.append(Prop_2Eventos[-1]*Italy_2['Eventos_masivos'][Italy_1.index[0]+i] )\n",
    "Prop_2Alarma = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_2Alarma.append(Prop_2Alarma[-1]*Italy_2['Estado_de_Alarma'][Italy_1.index[0]+i])\n",
    "Prop_2Transprte = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_2Transprte.append(Prop_2Transprte[-1]*Italy_2['Transporte_publico'][Italy_1.index[0]+i])\n",
    "Prop_2Ejercicio = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_2Ejercicio.append(Prop_2Ejercicio[-1]*Italy_2['Ejercicio_en_publico'][Italy_1.index[0]+i])\n",
    "Prop_2Tiendas = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_2Tiendas.append(Prop_2Tiendas[-1]*Italy_2['Tiendas_abiertas_al_publico'][Italy_1.index[0]+i])\n",
    "Prop_2Medical = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_2Medical.append(Prop_2Medical[-1]*Italy_2['Medical_Quality'][Italy_1.index[0]+i])    \n",
    "    \n",
    "Italy_2.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "Italy_2['Clases'] = Prop_2Clases\n",
    "Italy_2['Eventos_masivos'] = Prop_2Eventos\n",
    "Italy_2['Estado_de_Alarma'] = Prop_2Alarma\n",
    "Italy_2['Transporte_publico'] = Prop_2Transprte\n",
    "Italy_2['Ejercicio_en_publico'] = Prop_2Ejercicio\n",
    "Italy_2['Tiendas_abiertas_al_publico'] = Prop_2Tiendas\n",
    "Italy_2['Medical_Quality'] = Prop_2Medical\n",
    "\n",
    "    \n",
    "    \n",
    "Prop_3Clases = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_3Clases.append(Prop_3Clases[-1]*Italy_3['Clases'][Italy_1.index[0]+i])\n",
    "Prop_3Eventos = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_3Eventos.append(Prop_3Eventos[-1]*Italy_3['Eventos_masivos'][Italy_1.index[0]+i])\n",
    "Prop_3Alarma = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_3Alarma.append(Prop_3Alarma[-1]*Italy_3['Estado_de_Alarma'][Italy_1.index[0]+i])\n",
    "Prop_3Transprte = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_3Transprte.append(Prop_3Transprte[-1]*Italy_3['Transporte_publico'][Italy_1.index[0]+i])\n",
    "Prop_3Ejercicio = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_3Ejercicio.append(Prop_3Ejercicio[-1]*Italy_3['Ejercicio_en_publico'][Italy_1.index[0]+i])\n",
    "Prop_3Tiendas = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_3Tiendas.append(Prop_3Tiendas[-1]*Italy_3['Tiendas_abiertas_al_publico'][Italy_1.index[0]+i])\n",
    "Prop_3Medical = [10]\n",
    "for i in np.arange(0, len(Italy_3)-1):\n",
    "    Prop_3Medical.append(Prop_3Medical[-1]*Italy_3['Medical_Quality'][Italy_1.index[0]+i])    \n",
    "    \n",
    "Italy_3.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "Italy_3['Clases'] = Prop_3Clases\n",
    "Italy_3['Eventos_masivos'] = Prop_3Eventos\n",
    "Italy_3['Estado_de_Alarma'] = Prop_3Alarma\n",
    "Italy_3['Transporte_publico'] = Prop_3Transprte\n",
    "Italy_3['Ejercicio_en_publico'] = Prop_3Ejercicio\n",
    "Italy_3['Tiendas_abiertas_al_publico'] = Prop_3Tiendas\n",
    "Italy_3['Medical_Quality'] = Prop_3Medical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "UK = UKCases[UKCases.index>(UKCases[UKCases['total_deaths']>5].index-10)[0]]\n",
    "\n",
    "\n",
    "\n",
    "R0_1_UK = [2.5]\n",
    "R0_2_UK = [2]\n",
    "R0_3_UK = [1.5]\n",
    "    \n",
    "\n",
    "UK_1 = UK\n",
    "\n",
    "UK_1['dia'] = np.arange(0, len(UK))\n",
    "UK_1['Clases'] = R0_1_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-18'])), 1), np.full((len(UK[UK['date']>'2020-03-18'])), 0.25)]) \n",
    "EventosUK = np.hstack([np.full((len(UK[UK['date']<='2020-03-13'])), 1), np.full((len(UK[(UK['date']>'2020-03-13')&(UK['date']<='2020-03-20')])), 0.5)]) \n",
    "UK_1['Eventos_masivos'] = R0_1_UK*np.hstack([EventosUK, np.full((len(UK[UK['date']>'2020-03-20'])), 0.25)]) \n",
    "AlarmaUK = np.hstack([np.full((len(UK[UK['date']<='2020-03-15'])), 1), np.full((len(UK[(UK['date']>'2020-03-15')&(UK['date']<='2020-03-23')])), 0.5)])\n",
    "UK_1['Estado_de_Alarma'] = R0_1_UK*np.hstack([AlarmaUK, np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_1['Transporte_publico'] = R0_1_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-23'])), 1), np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_1['Ejercicio_en_publico'] = R0_1_UK*np.hstack([np.full((len(UK[UK['date']>'2020-02-01'])), 1)]) \n",
    "UK_1['Tiendas_abiertas_al_publico'] = R0_1_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-23'])), 1), np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_1['Medical_Quality'] = R0_1_UK*np.hstack([np.full((len(UK)), 0.5)])\n",
    "UK_1 = UK_1.fillna(0)\n",
    "\n",
    "UK_2 = UK\n",
    "\n",
    "UK_2['dia'] = np.arange(0, len(UK))\n",
    "UK_2['Clases'] = R0_2_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-18'])), 1), np.full((len(UK[UK['date']>'2020-03-18'])), 0.25)]) \n",
    "EventosUK = np.hstack([np.full((len(UK[UK['date']<='2020-03-13'])), 1), np.full((len(UK[(UK['date']>'2020-03-13')&(UK['date']<='2020-03-20')])), 0.5)]) \n",
    "UK_2['Eventos_masivos'] = R0_2_UK*np.hstack([EventosUK, np.full((len(UK[UK['date']>'2020-03-20'])), 0.25)]) \n",
    "AlarmaUK = np.hstack([np.full((len(UK[UK['date']<='2020-03-15'])), 1), np.full((len(UK[(UK['date']>'2020-03-15')&(UK['date']<='2020-03-23')])), 0.5)])\n",
    "UK_2['Estado_de_Alarma'] = R0_2_UK*np.hstack([AlarmaUK, np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_2['Transporte_publico'] = R0_2_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-23'])), 1), np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_2['Ejercicio_en_publico'] = R0_2_UK*np.hstack([np.full((len(UK[UK['date']>'2020-02-01'])), 1)]) \n",
    "UK_2['Tiendas_abiertas_al_publico'] = R0_2_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-23'])), 1), np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_2['Medical_Quality'] = R0_2_UK*np.hstack([np.full((len(UK)), 0.5)])\n",
    "UK_2 = UK_2.fillna(0)\n",
    "\n",
    "UK_3 = UK\n",
    "\n",
    "\n",
    "UK_3['dia'] = np.arange(0, len(UK))\n",
    "UK_3['Clases'] = R0_3_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-18'])), 1), np.full((len(UK[UK['date']>'2020-03-18'])), 0.25)]) \n",
    "EventosUK = np.hstack([np.full((len(UK[UK['date']<='2020-03-13'])), 1), np.full((len(UK[(UK['date']>'2020-03-13')&(UK['date']<='2020-03-20')])), 0.5)]) \n",
    "UK_3['Eventos_masivos'] = R0_3_UK*np.hstack([EventosUK, np.full((len(UK[UK['date']>'2020-03-20'])), 0.25)]) \n",
    "AlarmaUK = np.hstack([np.full((len(UK[UK['date']<='2020-03-15'])), 1), np.full((len(UK[(UK['date']>'2020-03-15')&(UK['date']<='2020-03-23')])), 0.5)])\n",
    "UK_3['Estado_de_Alarma'] = R0_3_UK*np.hstack([AlarmaUK, np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_3['Transporte_publico'] = R0_3_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-23'])), 1), np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_3['Ejercicio_en_publico'] = R0_3_UK*np.hstack([np.full((len(UK[UK['date']>'2020-02-01'])), 1)]) \n",
    "UK_3['Tiendas_abiertas_al_publico'] = R0_3_UK*np.hstack([np.full((len(UK[UK['date']<='2020-03-23'])), 1), np.full((len(UK[UK['date']>'2020-03-23'])), 0.25)]) \n",
    "UK_3['Medical_Quality'] = R0_3_UK*np.hstack([np.full((len(UK)), 0.5)])\n",
    "UK_3 = UK_3.fillna(0)\n",
    "\n",
    "\n",
    "Prop_1Clases = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_1Clases.append(Prop_1Clases[-1]*UK_1['Clases'][UK_1.index[0]+i])\n",
    "Prop_1Eventos = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_1Eventos.append(Prop_1Eventos[-1]*UK_1['Eventos_masivos'][UK_1.index[0]+i] )\n",
    "Prop_1Alarma = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_1Alarma.append(Prop_1Alarma[-1]*UK_1['Estado_de_Alarma'][UK_1.index[0]+i])\n",
    "Prop_1Transprte = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_1Transprte.append(Prop_1Transprte[-1]*UK_1['Transporte_publico'][UK_1.index[0]+i])\n",
    "Prop_1Ejercicio = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_1Ejercicio.append(Prop_1Ejercicio[-1]*UK_1['Ejercicio_en_publico'][UK_1.index[0]+i])\n",
    "Prop_1Tiendas = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_1Tiendas.append(Prop_1Tiendas[-1]*UK_1['Tiendas_abiertas_al_publico'][UK_1.index[0]+i])\n",
    "Prop_1Medical = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_1Medical.append(Prop_1Medical[-1]*UK_1['Medical_Quality'][UK_1.index[0]+i])    \n",
    "    \n",
    "UK_1.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "UK_1['Clases'] = Prop_1Clases\n",
    "UK_1['Eventos_masivos'] = Prop_1Eventos\n",
    "UK_1['Estado_de_Alarma'] = Prop_1Alarma\n",
    "UK_1['Transporte_publico'] = Prop_1Transprte\n",
    "UK_1['Ejercicio_en_publico'] = Prop_1Ejercicio\n",
    "UK_1['Tiendas_abiertas_al_publico'] = Prop_1Tiendas\n",
    "UK_1['Medical_Quality'] = Prop_1Medical\n",
    "\n",
    "    \n",
    "    \n",
    "Prop_2Clases = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_2Clases.append(Prop_2Clases[-1]*UK_2['Clases'][UK_1.index[0]+i])\n",
    "Prop_2Eventos = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_2Eventos.append(Prop_2Eventos[-1]*UK_2['Eventos_masivos'][UK_1.index[0]+i] )\n",
    "Prop_2Alarma = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_2Alarma.append(Prop_2Alarma[-1]*UK_2['Estado_de_Alarma'][UK_1.index[0]+i])\n",
    "Prop_2Transprte = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_2Transprte.append(Prop_2Transprte[-1]*UK_2['Transporte_publico'][UK_1.index[0]+i])\n",
    "Prop_2Ejercicio = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_2Ejercicio.append(Prop_2Ejercicio[-1]*UK_2['Ejercicio_en_publico'][UK_1.index[0]+i])\n",
    "Prop_2Tiendas = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_2Tiendas.append(Prop_2Tiendas[-1]*UK_2['Tiendas_abiertas_al_publico'][UK_1.index[0]+i])\n",
    "Prop_2Medical = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_2Medical.append(Prop_2Medical[-1]*UK_2['Medical_Quality'][UK_1.index[0]+i])    \n",
    "    \n",
    "UK_2.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "UK_2['Clases'] = Prop_2Clases\n",
    "UK_2['Eventos_masivos'] = Prop_2Eventos\n",
    "UK_2['Estado_de_Alarma'] = Prop_2Alarma\n",
    "UK_2['Transporte_publico'] = Prop_2Transprte\n",
    "UK_2['Ejercicio_en_publico'] = Prop_2Ejercicio\n",
    "UK_2['Tiendas_abiertas_al_publico'] = Prop_2Tiendas\n",
    "UK_2['Medical_Quality'] = Prop_2Medical\n",
    "\n",
    "    \n",
    "    \n",
    "Prop_3Clases = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_3Clases.append(Prop_3Clases[-1]*UK_3['Clases'][UK_1.index[0]+i])\n",
    "Prop_3Eventos = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_3Eventos.append(Prop_3Eventos[-1]*UK_3['Eventos_masivos'][UK_1.index[0]+i])\n",
    "Prop_3Alarma = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_3Alarma.append(Prop_3Alarma[-1]*UK_3['Estado_de_Alarma'][UK_1.index[0]+i])\n",
    "Prop_3Transprte = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_3Transprte.append(Prop_3Transprte[-1]*UK_3['Transporte_publico'][UK_1.index[0]+i])\n",
    "Prop_3Ejercicio = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_3Ejercicio.append(Prop_3Ejercicio[-1]*UK_3['Ejercicio_en_publico'][UK_1.index[0]+i])\n",
    "Prop_3Tiendas = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_3Tiendas.append(Prop_3Tiendas[-1]*UK_3['Tiendas_abiertas_al_publico'][UK_1.index[0]+i])\n",
    "Prop_3Medical = [10]\n",
    "for i in np.arange(0, len(UK_3)-1):\n",
    "    Prop_3Medical.append(Prop_3Medical[-1]*UK_3['Medical_Quality'][UK_1.index[0]+i])    \n",
    "    \n",
    "UK_3.drop(['Clases', 'Eventos_masivos', 'Eventos_masivos', 'Estado_de_Alarma', 'Transporte_publico', 'Ejercicio_en_publico', 'Tiendas_abiertas_al_publico', 'Medical_Quality'], axis=1, inplace=True)            \n",
    "UK_3['Clases'] = Prop_3Clases\n",
    "UK_3['Eventos_masivos'] = Prop_3Eventos\n",
    "UK_3['Estado_de_Alarma'] = Prop_3Alarma\n",
    "UK_3['Transporte_publico'] = Prop_3Transprte\n",
    "UK_3['Ejercicio_en_publico'] = Prop_3Ejercicio\n",
    "UK_3['Tiendas_abiertas_al_publico'] = Prop_3Tiendas\n",
    "UK_3['Medical_Quality'] = Prop_3Medical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X and Y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Training = Italy + China + Spain + part England\n",
    " \n",
    " Test = part England\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a sumir 3 factores diferenciados. \n",
    "\n",
    "Primero, el tiempo pasado desde el que suponemos momento 0 (15 días antes del primer día de alcanzar 15 muertes)\n",
    "\n",
    "\n",
    "Por un lado los factores de propagación (los R0). Estos se verán afectados por las diferentes medidas de cuarentena que se tomen, por lo que multiplicamos los valores de cuarentena por los valores de propagación e irán entre 15 y 20 días de retraso con respecto a las muertes.\n",
    "\n",
    "Luego tendremos los casos analizados, que no son un valor demasiado fiable dado la diferencia de tests realizados en unos y otros países, pero la tomaremos igualmente a falta de otros factores que facilitarían esto (número de tests). Podría multiplicarse esta columna por porcentaje de positivos en tests por país para intentar calibrarlo. Estas columnas irán con entre 10 y 5 días de retraso respecto a las muertes.\n",
    "\n",
    "Por último nuestra 'Y' serán las muertes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackeo de datos de propagación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_1_1RO_1 = np.vstack([China_1.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_1.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "Stack_1_1RO_2 = np.vstack([China_2.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_2.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "Stack_1_1RO_3 = np.vstack([China_3.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_3.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "\n",
    "\n",
    "Stack_1_2RO_1 =  Spain_1.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "Stack_1_2RO_2 =  Spain_2.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "Stack_1_2RO_3 =  Spain_3.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "\n",
    "\n",
    "Stack_1_3RO_1 = np.vstack((Stack_1_1RO_1,Stack_1_2RO_1))\n",
    "Stack_1_3RO_2 = np.vstack((Stack_1_1RO_2,Stack_1_2RO_2))\n",
    "Stack_1_3RO_3 = np.vstack((Stack_1_1RO_3,Stack_1_2RO_3))\n",
    "\n",
    "\n",
    "Stack_1_4RO_1 =  UK_1.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "Stack_1_4RO_2 =  UK_2.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "Stack_1_4RO_3 =  UK_3.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "\n",
    "Stack_1_5RO_1 = np.vstack((Stack_1_3RO_1,Stack_1_4RO_1))\n",
    "Stack_1_5RO_2 = np.vstack((Stack_1_3RO_2,Stack_1_4RO_2))\n",
    "Stack_1_5RO_3 = np.vstack((Stack_1_3RO_3,Stack_1_4RO_3))\n",
    "\n",
    "\n",
    "\n",
    "Stack_2_1RO_1 = np.vstack([China_1.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_1.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "Stack_2_1RO_2 = np.vstack([China_2.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_2.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "Stack_2_1RO_3 = np.vstack([China_3.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_3.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "\n",
    "\n",
    "Stack_2_2RO_1 =  Spain_1.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "Stack_2_2RO_2 =  Spain_2.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "Stack_2_2RO_3 =  Spain_3.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "\n",
    "\n",
    "Stack_2_3RO_1 = np.vstack((Stack_2_1RO_1,Stack_2_2RO_1))\n",
    "Stack_2_3RO_2 = np.vstack((Stack_2_1RO_2,Stack_2_2RO_2))\n",
    "Stack_2_3RO_3 = np.vstack((Stack_2_1RO_3,Stack_2_2RO_3))\n",
    "\n",
    "\n",
    "Stack_2_4RO_1 =  UK_1.iloc[10:-10, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "Stack_2_4RO_2 =  UK_2.iloc[10:-10, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "Stack_2_4RO_3 =  UK_3.iloc[10:-10, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "\n",
    "Stack_2_5RO_1 = np.vstack((Stack_2_3RO_1,Stack_2_4RO_1))\n",
    "Stack_2_5RO_2 = np.vstack((Stack_2_3RO_2,Stack_2_4RO_2))\n",
    "Stack_2_5RO_3 = np.vstack((Stack_2_3RO_3,Stack_2_4RO_3))\n",
    "\n",
    "\n",
    "Prop1 = np.hstack((Stack_1_5RO_1, Stack_2_5RO_1))\n",
    "Prop2 = np.hstack((Stack_1_5RO_2, Stack_2_5RO_2))\n",
    "Prop3 = np.hstack((Stack_1_5RO_3, Stack_2_5RO_3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackeo de datos de muertes/M totales - 5/10 días antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_3_1RO_1 = np.vstack([China_1.iloc[10:-10, [4]].values/60, Italy_1.iloc[10:-10, [4]].values/61])\n",
    "Stack_3_1RO_2 = np.vstack([China_2.iloc[10:-10, [4]].values/60, Italy_2.iloc[10:-10, [4]].values/61])\n",
    "Stack_3_1RO_3 = np.vstack([China_3.iloc[10:-10, [4]].values/60, Italy_3.iloc[10:-10, [4]].values/61])\n",
    "\n",
    "\n",
    "Stack_3_2RO_1 =  Spain_1.iloc[10:-10, [4]].values/47\n",
    "Stack_3_2RO_2 =  Spain_2.iloc[10:-10, [4]].values/47\n",
    "Stack_3_2RO_3 =  Spain_3.iloc[10:-10, [4]].values/47\n",
    "\n",
    "\n",
    "Stack_3_3RO_1 = np.vstack((Stack_3_1RO_1,Stack_3_2RO_1))\n",
    "Stack_3_3RO_2 = np.vstack((Stack_3_1RO_2,Stack_3_2RO_2))\n",
    "Stack_3_3RO_3 = np.vstack((Stack_3_1RO_3,Stack_3_2RO_3))\n",
    "\n",
    "\n",
    "Stack_3_4RO_1 =  UK_1.iloc[15:-5, [4]].values/67\n",
    "Stack_3_4RO_2 =  UK_2.iloc[15:-5, [4]].values/67\n",
    "Stack_3_4RO_3 =  UK_3.iloc[15:-5, [4]].values/67\n",
    "\n",
    "Stack_3_5RO_1 = np.vstack((Stack_3_3RO_1,Stack_3_4RO_1))\n",
    "Stack_3_5RO_2 = np.vstack((Stack_3_3RO_2,Stack_3_4RO_2))\n",
    "Stack_3_5RO_3 = np.vstack((Stack_3_3RO_3,Stack_3_4RO_3))\n",
    "\n",
    "\n",
    "\n",
    "Stack_4_1RO_1 = np.vstack([China_1.iloc[15:-5, [4]].values/60, Italy_1.iloc[15:-5, [4]].values/61])\n",
    "Stack_4_1RO_2 = np.vstack([China_2.iloc[15:-5, [4]].values/60, Italy_2.iloc[15:-5, [4]].values/61])\n",
    "Stack_4_1RO_3 = np.vstack([China_3.iloc[15:-5, [4]].values/60, Italy_3.iloc[15:-5, [4]].values/61])\n",
    "\n",
    "\n",
    "Stack_4_2RO_1 =  Spain_1.iloc[15:-5, [4]].values/47\n",
    "Stack_4_2RO_2 =  Spain_2.iloc[15:-5, [4]].values/47\n",
    "Stack_4_2RO_3 =  Spain_3.iloc[15:-5, [4]].values/47\n",
    "\n",
    "\n",
    "Stack_4_3RO_1 = np.vstack((Stack_4_1RO_1,Stack_4_2RO_1))\n",
    "Stack_4_3RO_2 = np.vstack((Stack_4_1RO_2,Stack_4_2RO_2))\n",
    "Stack_4_3RO_3 = np.vstack((Stack_4_1RO_3,Stack_4_2RO_3))\n",
    "\n",
    "\n",
    "Stack_4_4RO_1 =  UK_1.iloc[20:, [4]].values/67\n",
    "Stack_4_4RO_2 =  UK_2.iloc[20:, [4]].values/67\n",
    "Stack_4_4RO_3 =  UK_3.iloc[20:, [4]].values/67\n",
    "\n",
    "Stack_4_5RO_1 = np.vstack((Stack_4_3RO_1,Stack_4_4RO_1))\n",
    "Stack_4_5RO_2 = np.vstack((Stack_4_3RO_2,Stack_4_4RO_2))\n",
    "Stack_4_5RO_3 = np.vstack((Stack_4_3RO_3,Stack_4_4RO_3))\n",
    "\n",
    "\n",
    "Cases1 = np.hstack((Stack_3_5RO_1, Stack_4_5RO_1))\n",
    "Cases2 = np.hstack((Stack_3_5RO_2, Stack_4_5RO_2))\n",
    "Cases3 = np.hstack((Stack_3_5RO_3, Stack_4_5RO_3))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackeo de días"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_5_1RO_1 = np.vstack([China_1.iloc[20:, [5]].values, Italy_1.iloc[20:, [5]].values])\n",
    "Stack_5_1RO_2 = np.vstack([China_2.iloc[20:, [5]].values, Italy_2.iloc[20:, [5]].values])\n",
    "Stack_5_1RO_3 = np.vstack([China_3.iloc[20:, [5]].values, Italy_3.iloc[20:, [5]].values])\n",
    "\n",
    "\n",
    "Stack_5_2RO_1 =  Spain_1.iloc[20:, [5]].values\n",
    "Stack_5_2RO_2 =  Spain_2.iloc[20:, [5]].values\n",
    "Stack_5_2RO_3 =  Spain_3.iloc[20:, [5]].values\n",
    "\n",
    "\n",
    "Stack_5_3RO_1 = np.vstack((Stack_5_1RO_1,Stack_5_2RO_1))\n",
    "Stack_5_3RO_2 = np.vstack((Stack_5_1RO_2,Stack_5_2RO_2))\n",
    "Stack_5_3RO_3 = np.vstack((Stack_5_1RO_3,Stack_5_2RO_3))\n",
    "\n",
    "\n",
    "Stack_5_4RO_1 =  UK_1.iloc[20:, [5]].values\n",
    "Stack_5_4RO_2 =  UK_2.iloc[20:, [5]].values\n",
    "Stack_5_4RO_3 =  UK_3.iloc[20:, [5]].values\n",
    "\n",
    "Days1 = np.vstack((Stack_5_3RO_1,Stack_5_4RO_1))\n",
    "Days2 = np.vstack((Stack_5_3RO_2,Stack_5_4RO_2))\n",
    "Days3 = np.vstack((Stack_5_3RO_3,Stack_5_4RO_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackeo conjunto de la X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stackeo1 = np.log10(np.hstack((Prop1, Cases1)))\n",
    "Stackeo2 = np.log10(np.hstack((Prop2, Cases2)))\n",
    "Stackeo2 = np.log10(np.hstack((Prop3, Cases3)))\n",
    "\n",
    "x1 = np.hstack((Stackeo1, Days1))\n",
    "x2 = np.hstack((Stackeo2, Days2))\n",
    "x3 = np.hstack((Stackeo2, Days3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackeo Y (muertes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 16)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stackeo1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_6_1RO_1 = np.vstack([China_1.iloc[20:, [2]].values/60, Italy_1.iloc[20:, [2]].values/61])\n",
    "Stack_6_1RO_2 = np.vstack([China_2.iloc[20:, [2]].values/60, Italy_2.iloc[20:, [2]].values/61])\n",
    "Stack_6_1RO_3 = np.vstack([China_3.iloc[20:, [2]].values/60, Italy_3.iloc[20:, [2]].values/61])\n",
    "\n",
    "\n",
    "Stack_6_2RO_1 =  Spain_1.iloc[20:, [2]].values/47\n",
    "Stack_6_2RO_2 =  Spain_2.iloc[20:, [2]].values/47\n",
    "Stack_6_2RO_3 =  Spain_3.iloc[20:, [2]].values/47\n",
    "\n",
    "\n",
    "Stack_6_3RO_1 = np.vstack((Stack_6_1RO_1,Stack_6_2RO_1))\n",
    "Stack_6_3RO_2 = np.vstack((Stack_6_1RO_2,Stack_6_2RO_2))\n",
    "Stack_6_3RO_3 = np.vstack((Stack_6_1RO_3,Stack_6_2RO_3))\n",
    "\n",
    "\n",
    "Stack_6_4RO_1 =  UK_1.iloc[20:, [2]].values/67\n",
    "Stack_6_4RO_2 =  UK_2.iloc[20:, [2]].values/67\n",
    "Stack_6_4RO_3 =  UK_3.iloc[20:, [2]].values/67\n",
    "\n",
    "Y1 = np.log10(np.vstack((Stack_6_3RO_1,Stack_6_4RO_1)))\n",
    "Y2 = np.log10(np.vstack((Stack_6_3RO_2,Stack_6_4RO_2)))\n",
    "Y3 = np.log10(np.vstack((Stack_6_3RO_3,Stack_6_4RO_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestra X y nuestra Y podemos realizar un análisis de los componentes principales para reducir el ruido. Nuestra X tiene 15 factores, pero muchos de ellos son redundantes, podemos probar a reducirlo a entre 10 y 8 por ejemplo. También normalizamos esos factores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing training and test + Feature selection 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/adelgado/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "x1[x1 == -inf] = 0\n",
    "x2[x2 == -inf] = 0\n",
    "x3[x3 == -inf] = 0\n",
    "\n",
    "Y1[Y1 == -inf] = 0\n",
    "Y2[Y2 == -inf] = 0\n",
    "Y3[Y3 == -inf] = 0\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler1 = MinMaxScaler()\n",
    "X1 = min_max_scaler1.fit_transform(x1)\n",
    "\n",
    "min_max_scaler2 = MinMaxScaler()\n",
    "X2 = min_max_scaler2.fit_transform(x2)\n",
    "\n",
    "min_max_scaler3 = MinMaxScaler()\n",
    "X3 = min_max_scaler3.fit_transform(x3)\n",
    "\n",
    "min_max_scaler4 = MinMaxScaler()\n",
    "Y1 = min_max_scaler4.fit_transform(Y1)\n",
    "\n",
    "min_max_scaler5 = MinMaxScaler()\n",
    "Y2 = min_max_scaler5.fit_transform(Y2)\n",
    "\n",
    "min_max_scaler6 = MinMaxScaler()\n",
    "Y3 = min_max_scaler6.fit_transform(Y3)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "RF = ExtraTreesRegressor()\n",
    "RF.fit(X1, Y1)\n",
    "featureselector1 = SelectFromModel(RF, prefit=True)\n",
    "X1 = featureselector1.transform(X1)\n",
    "\n",
    "RF = ExtraTreesRegressor()\n",
    "RF.fit(X2, Y2)\n",
    "featureselector2 = SelectFromModel(RF, prefit=True)\n",
    "X2 = featureselector2.transform(X2)\n",
    "\n",
    "RF = ExtraTreesRegressor()\n",
    "RF.fit(X3, Y3)\n",
    "featureselector3 = SelectFromModel(RF, prefit=True)\n",
    "X3 = featureselector3.transform(X3)\n",
    "\n",
    "Shape1 = X1.shape[1]\n",
    "Shape2 = X2.shape[1]\n",
    "Shape3 = X3.shape[1]\n",
    "\n",
    "X1_train = X1[:-10].reshape(len(X1)-10, 1, X1.shape[1])\n",
    "X2_train = X2[:-10].reshape(len(X2)-10, 1, X2.shape[1])\n",
    "X3_train = X3[:-10].reshape(len(X3)-10, 1, X3.shape[1])\n",
    "\n",
    "X1_test = X1[-10:].reshape(10, 1, X1.shape[1])\n",
    "X2_test = X2[-10:].reshape(10, 1, X2.shape[1])\n",
    "X3_test = X3[-10:].reshape(10, 1, X3.shape[1])\n",
    "\n",
    "\n",
    "Y1_train = Y1[:-10]\n",
    "Y2_train = Y2[:-10]\n",
    "Y3_train = Y3[:-10]\n",
    "\n",
    "Y1_test = Y1[-10:]\n",
    "Y2_test = Y2[-10:]\n",
    "Y3_test = Y3[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.3027 - mean_absolute_error: 0.3027\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1263 - mean_absolute_error: 0.1263\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1035 - mean_absolute_error: 0.1035\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0928 - mean_absolute_error: 0.0928\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1096 - mean_absolute_error: 0.1096\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0778 - mean_absolute_error: 0.0778\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0866 - mean_absolute_error: 0.0866\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0912 - mean_absolute_error: 0.0912\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0778 - mean_absolute_error: 0.0778\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0796 - mean_absolute_error: 0.0796\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0820 - mean_absolute_error: 0.0820\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0841 - mean_absolute_error: 0.0841\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0785 - mean_absolute_error: 0.0785\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0700 - mean_absolute_error: 0.0700\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0759 - mean_absolute_error: 0.0759\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0711 - mean_absolute_error: 0.0711\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0656 - mean_absolute_error: 0.0656\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0728 - mean_absolute_error: 0.0728\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0670 - mean_absolute_error: 0.0670\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0699 - mean_absolute_error: 0.0699\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0636 - mean_absolute_error: 0.0636\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0706 - mean_absolute_error: 0.0706\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0703 - mean_absolute_error: 0.0703\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0673 - mean_absolute_error: 0.0673\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0625 - mean_absolute_error: 0.0625\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0654 - mean_absolute_error: 0.0654\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0642 - mean_absolute_error: 0.0642\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0625 - mean_absolute_error: 0.0625\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0658 - mean_absolute_error: 0.0658\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0664 - mean_absolute_error: 0.0664\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0690 - mean_absolute_error: 0.0690\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0666 - mean_absolute_error: 0.0666\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0615 - mean_absolute_error: 0.0615\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0611 - mean_absolute_error: 0.0611\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0718 - mean_absolute_error: 0.0718\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0550 - mean_absolute_error: 0.0550\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0734 - mean_absolute_error: 0.0734\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0728 - mean_absolute_error: 0.0728\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0669 - mean_absolute_error: 0.0669\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0640 - mean_absolute_error: 0.0640\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0679 - mean_absolute_error: 0.0679\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0590 - mean_absolute_error: 0.0590\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0567 - mean_absolute_error: 0.0567\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0640 - mean_absolute_error: 0.0640\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0667 - mean_absolute_error: 0.0667\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0526 - mean_absolute_error: 0.0526\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0569 - mean_absolute_error: 0.0569\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0604 - mean_absolute_error: 0.0604\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0567 - mean_absolute_error: 0.0567\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0675 - mean_absolute_error: 0.0675\n",
      "10/10 [==============================] - 0s 34ms/step\n",
      "Test score: 0.3375676732310019\n",
      "Accuracy: 0.3336277902126312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelLSTM01_1 = Sequential()\n",
    "modelLSTM01_1.add(LSTM(40, return_sequences=True))\n",
    "modelLSTM01_1.add(LSTM(200, activation='relu'))\n",
    "modelLSTM01_1.add(Dense(40, activation='relu'))\n",
    "modelLSTM01_1.add(Dense(10, activation='relu'))\n",
    "modelLSTM01_1.add(Dense(1, activation='relu'))\n",
    "modelLSTM01_1.compile(optimizer='Adamax', loss='mae', metrics = ['mae'])\n",
    "\n",
    "\n",
    "modelLSTM01_1.fit(X1_train, Y1_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "LSTMPredicton01_1 = modelLSTM01_1.predict(X1_test)\n",
    "score, mae = modelLSTM01_1.evaluate(X1_test, Y1_test)\n",
    "score = sqrt(mean_squared_error(LSTMPredicton01_1, Y1_test))\n",
    "print('Test score:', score)\n",
    "print('Accuracy:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "96/96 [==============================] - 1s 12ms/step - loss: 0.2242 - mean_absolute_error: 0.2242\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1314 - mean_absolute_error: 0.1314\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1225 - mean_absolute_error: 0.1225\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1083 - mean_absolute_error: 0.1083\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0978 - mean_absolute_error: 0.0978\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1101 - mean_absolute_error: 0.1101\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0881 - mean_absolute_error: 0.0881\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0985 - mean_absolute_error: 0.0985\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0859 - mean_absolute_error: 0.0859\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1026 - mean_absolute_error: 0.1026\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0865 - mean_absolute_error: 0.0865\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0749 - mean_absolute_error: 0.0749\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0816 - mean_absolute_error: 0.0816\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0727 - mean_absolute_error: 0.0727\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0708 - mean_absolute_error: 0.0708\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0785 - mean_absolute_error: 0.0785\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0761 - mean_absolute_error: 0.0761\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0788 - mean_absolute_error: 0.0788\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0740 - mean_absolute_error: 0.0740\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0744 - mean_absolute_error: 0.0744\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0671 - mean_absolute_error: 0.0671\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0802 - mean_absolute_error: 0.0802\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0702 - mean_absolute_error: 0.0702\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0708 - mean_absolute_error: 0.0708\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0725 - mean_absolute_error: 0.0725\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0737 - mean_absolute_error: 0.0737\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0680 - mean_absolute_error: 0.0680\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0660 - mean_absolute_error: 0.0660\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0813 - mean_absolute_error: 0.0813\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0762 - mean_absolute_error: 0.0762\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0842 - mean_absolute_error: 0.0842\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0709 - mean_absolute_error: 0.0709\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0706 - mean_absolute_error: 0.0706\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0774 - mean_absolute_error: 0.0774\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0754 - mean_absolute_error: 0.0754\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0741 - mean_absolute_error: 0.0741\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0694 - mean_absolute_error: 0.0694\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0673 - mean_absolute_error: 0.0673\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0691 - mean_absolute_error: 0.0691\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0727 - mean_absolute_error: 0.0727\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0621 - mean_absolute_error: 0.0621\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0681 - mean_absolute_error: 0.0681\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0670 - mean_absolute_error: 0.0670\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0680 - mean_absolute_error: 0.0680\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0670 - mean_absolute_error: 0.0670\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0695 - mean_absolute_error: 0.0695\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0630 - mean_absolute_error: 0.0630\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0663 - mean_absolute_error: 0.0663\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0720 - mean_absolute_error: 0.0720\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0641 - mean_absolute_error: 0.0641\n",
      "10/10 [==============================] - 0s 16ms/step\n",
      "Test score: 0.12599340541582205\n",
      "Accuracy: 0.11425552517175674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelLSTM02_1 = Sequential()\n",
    "modelLSTM02_1.add(LSTM(40, return_sequences=True))\n",
    "modelLSTM02_1.add(LSTM(200, activation='relu'))\n",
    "modelLSTM02_1.add(Dense(40, activation='relu'))\n",
    "modelLSTM02_1.add(Dense(10, activation='relu'))\n",
    "modelLSTM02_1.add(Dense(1, activation='relu'))\n",
    "modelLSTM02_1.compile(optimizer='Adamax', loss='mae', metrics = ['mae'])\n",
    "\n",
    "\n",
    "modelLSTM02_1.fit(X2_train, Y2_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "LSTMPredicton02_1 = modelLSTM02_1.predict(X2_test)\n",
    "score, mae = modelLSTM02_1.evaluate(X2_test, Y2_test)\n",
    "score = sqrt(mean_squared_error(LSTMPredicton02_1, Y2_test))\n",
    "print('Test score:', score)\n",
    "print('Accuracy:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "96/96 [==============================] - 2s 19ms/step - loss: 0.2248 - mean_absolute_error: 0.2248\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1349 - mean_absolute_error: 0.1349\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1312 - mean_absolute_error: 0.1312\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1106 - mean_absolute_error: 0.1106\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1038 - mean_absolute_error: 0.1038\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1014 - mean_absolute_error: 0.1014\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0917 - mean_absolute_error: 0.0917\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0870 - mean_absolute_error: 0.0870\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0848 - mean_absolute_error: 0.0848\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0851 - mean_absolute_error: 0.0851\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0845 - mean_absolute_error: 0.0845\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0728 - mean_absolute_error: 0.0728\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0847 - mean_absolute_error: 0.0847\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0762 - mean_absolute_error: 0.0762\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0735 - mean_absolute_error: 0.0735\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0860 - mean_absolute_error: 0.0860\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0717 - mean_absolute_error: 0.0717\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0748 - mean_absolute_error: 0.0748\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0749 - mean_absolute_error: 0.0749\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0671 - mean_absolute_error: 0.0671\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0928 - mean_absolute_error: 0.0928\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0716 - mean_absolute_error: 0.0716\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0624 - mean_absolute_error: 0.0624\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0653 - mean_absolute_error: 0.0653\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0832 - mean_absolute_error: 0.0832\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0718 - mean_absolute_error: 0.0718\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0710 - mean_absolute_error: 0.0710\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0580 - mean_absolute_error: 0.0580\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0563 - mean_absolute_error: 0.0563\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0746 - mean_absolute_error: 0.0746\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0713 - mean_absolute_error: 0.0713\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0699 - mean_absolute_error: 0.0699\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0629 - mean_absolute_error: 0.0629\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0730 - mean_absolute_error: 0.0730\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0669 - mean_absolute_error: 0.0669\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0557 - mean_absolute_error: 0.0557\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0728 - mean_absolute_error: 0.0728\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0609 - mean_absolute_error: 0.0609\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0572 - mean_absolute_error: 0.0572\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0603 - mean_absolute_error: 0.0603\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0611 - mean_absolute_error: 0.0611\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0570 - mean_absolute_error: 0.0570\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0733 - mean_absolute_error: 0.0733\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0644 - mean_absolute_error: 0.0644\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0656 - mean_absolute_error: 0.0656\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0579 - mean_absolute_error: 0.0579\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0669 - mean_absolute_error: 0.0669\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0637 - mean_absolute_error: 0.0637\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0594 - mean_absolute_error: 0.0594\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0692 - mean_absolute_error: 0.0692\n",
      "10/10 [==============================] - 0s 42ms/step\n",
      "Test score: 0.1967802544958769\n",
      "Accuracy: 0.1898816078901291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelLSTM03_1 = Sequential()\n",
    "modelLSTM03_1.add(LSTM(40, return_sequences=True))\n",
    "modelLSTM03_1.add(LSTM(200, activation='relu'))\n",
    "modelLSTM03_1.add(Dense(40, activation='relu'))\n",
    "modelLSTM03_1.add(Dense(10, activation='relu'))\n",
    "modelLSTM03_1.add(Dense(1, activation='relu'))\n",
    "modelLSTM03_1.compile(optimizer='Adamax', loss='mae', metrics = ['mae'])\n",
    "\n",
    "\n",
    "modelLSTM03_1.fit(X3_train, Y3_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "LSTMPredicton03_1 = modelLSTM03_1.predict(X3_test)\n",
    "score, mae = modelLSTM03_1.evaluate(X3_test, Y3_test)\n",
    "score = sqrt(mean_squared_error(LSTMPredicton03_1, Y3_test))\n",
    "print('Test score:', score)\n",
    "print('Accuracy:', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSMprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "96/96 [==============================] - 2s 25ms/step - loss: 0.1940 - mean_absolute_error: 0.1940\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.1013 - mean_absolute_error: 0.1013\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0977 - mean_absolute_error: 0.0977\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0877 - mean_absolute_error: 0.0877\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0861 - mean_absolute_error: 0.0861\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0843 - mean_absolute_error: 0.0843\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0825 - mean_absolute_error: 0.0825\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0825 - mean_absolute_error: 0.0825\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0793 - mean_absolute_error: 0.0793\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0718 - mean_absolute_error: 0.0718\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0769 - mean_absolute_error: 0.0769\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0763 - mean_absolute_error: 0.0763\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0680 - mean_absolute_error: 0.0680\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0808 - mean_absolute_error: 0.0808\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0761 - mean_absolute_error: 0.0761\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0723 - mean_absolute_error: 0.0723\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0730 - mean_absolute_error: 0.0730\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0685 - mean_absolute_error: 0.0685\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0726 - mean_absolute_error: 0.0726\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0710 - mean_absolute_error: 0.0710\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0742 - mean_absolute_error: 0.0742\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0708 - mean_absolute_error: 0.0708\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0764 - mean_absolute_error: 0.0764\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0637 - mean_absolute_error: 0.0637\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0732 - mean_absolute_error: 0.0732\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0672 - mean_absolute_error: 0.0672\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0700 - mean_absolute_error: 0.0700\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0670 - mean_absolute_error: 0.0670\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0661 - mean_absolute_error: 0.0661\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0653 - mean_absolute_error: 0.0653\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0645 - mean_absolute_error: 0.0645\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0650 - mean_absolute_error: 0.0650\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0644 - mean_absolute_error: 0.0644\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0600 - mean_absolute_error: 0.0600\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0658 - mean_absolute_error: 0.0658\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0640 - mean_absolute_error: 0.0640\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0676 - mean_absolute_error: 0.0676\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0639 - mean_absolute_error: 0.0639\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0668 - mean_absolute_error: 0.0668\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0631 - mean_absolute_error: 0.0631\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0628 - mean_absolute_error: 0.0628\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0593 - mean_absolute_error: 0.0593\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0636 - mean_absolute_error: 0.0636\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0612 - mean_absolute_error: 0.0612\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0581 - mean_absolute_error: 0.0581\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0616 - mean_absolute_error: 0.0616\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0597 - mean_absolute_error: 0.0597\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0623 - mean_absolute_error: 0.0623\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0606 - mean_absolute_error: 0.0606\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0592 - mean_absolute_error: 0.0592\n",
      "10/10 [==============================] - 1s 69ms/step\n",
      "Test score: 0.2769758046567387\n",
      "Accuracy: 0.27251601219177246\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelLSTM01_2 = Sequential()\n",
    "modelLSTM01_2.add(LSTM(40, return_sequences=True))\n",
    "modelLSTM01_2.add(LSTM(200, activation='relu'))\n",
    "modelLSTM01_2.add(Dense(40, activation='relu'))\n",
    "modelLSTM01_2.add(Dense(10, activation='relu'))\n",
    "modelLSTM01_2.add(Dense(1, activation='relu'))\n",
    "modelLSTM01_2.compile(optimizer='RMSProp', loss='mae', metrics = ['mae'])\n",
    "\n",
    "\n",
    "modelLSTM01_2.fit(X1_train, Y1_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "LSTMPredicton01_2 = modelLSTM01_2.predict(X1_test)\n",
    "score, mae = modelLSTM01_2.evaluate(X1_test, Y1_test)\n",
    "score = sqrt(mean_squared_error(LSTMPredicton01_2, Y1_test))\n",
    "print('Test score:', score)\n",
    "print('Accuracy:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "96/96 [==============================] - 2s 21ms/step - loss: 0.5816 - mean_absolute_error: 0.5816\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1738 - mean_absolute_error: 0.1738\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1281 - mean_absolute_error: 0.1281\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1165 - mean_absolute_error: 0.1165\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.1262 - mean_absolute_error: 0.1262\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.1099 - mean_absolute_error: 0.1099\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.1159 - mean_absolute_error: 0.1159\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.1047 - mean_absolute_error: 0.1047\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.1015 - mean_absolute_error: 0.1015\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0984 - mean_absolute_error: 0.0984\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.1059 - mean_absolute_error: 0.1059\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0995 - mean_absolute_error: 0.0995\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0923 - mean_absolute_error: 0.0923\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0908 - mean_absolute_error: 0.0908\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0956 - mean_absolute_error: 0.0956\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0829 - mean_absolute_error: 0.0829\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0833 - mean_absolute_error: 0.0833\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0874 - mean_absolute_error: 0.0874\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0834 - mean_absolute_error: 0.0834\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0852 - mean_absolute_error: 0.0852\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0804 - mean_absolute_error: 0.0804\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0824 - mean_absolute_error: 0.0824\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0735 - mean_absolute_error: 0.0735\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0779 - mean_absolute_error: 0.0779\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0802 - mean_absolute_error: 0.0802\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0826 - mean_absolute_error: 0.0826\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0759 - mean_absolute_error: 0.0759\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0752 - mean_absolute_error: 0.0752\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0796 - mean_absolute_error: 0.0796\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0786 - mean_absolute_error: 0.0786\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0771 - mean_absolute_error: 0.0771\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0731 - mean_absolute_error: 0.0731\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0769 - mean_absolute_error: 0.0769\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0763 - mean_absolute_error: 0.0763\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0688 - mean_absolute_error: 0.0688\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0703 - mean_absolute_error: 0.0703\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0706 - mean_absolute_error: 0.0706\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0731 - mean_absolute_error: 0.0731\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0677 - mean_absolute_error: 0.0677\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0748 - mean_absolute_error: 0.0748\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0704 - mean_absolute_error: 0.0704\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0690 - mean_absolute_error: 0.0690\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0757 - mean_absolute_error: 0.0757\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0707 - mean_absolute_error: 0.0707\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0761 - mean_absolute_error: 0.0761\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0701 - mean_absolute_error: 0.0701\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0704 - mean_absolute_error: 0.0704\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0711 - mean_absolute_error: 0.0711\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0715 - mean_absolute_error: 0.0715\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0688 - mean_absolute_error: 0.0688\n",
      "10/10 [==============================] - 1s 55ms/step\n",
      "Test score: 0.19739752710981534\n",
      "Accuracy: 0.19062432646751404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelLSTM02_2 = Sequential()\n",
    "modelLSTM02_2.add(LSTM(40, return_sequences=True))\n",
    "modelLSTM02_2.add(LSTM(200, activation='relu'))\n",
    "modelLSTM02_2.add(Dense(40, activation='relu'))\n",
    "modelLSTM02_2.add(Dense(10, activation='relu'))\n",
    "modelLSTM02_2.add(Dense(1, activation='relu'))\n",
    "modelLSTM02_2.compile(optimizer='RMSProp', loss='mae', metrics = ['mae'])\n",
    "\n",
    "\n",
    "modelLSTM02_2.fit(X2_train, Y2_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "LSTMPredicton02_2 = modelLSTM02_2.predict(X2_test)\n",
    "score, mae = modelLSTM02_2.evaluate(X2_test, Y2_test)\n",
    "score = sqrt(mean_squared_error(LSTMPredicton02_2, Y2_test))\n",
    "print('Test score:', score)\n",
    "print('Accuracy:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "96/96 [==============================] - 2s 22ms/step - loss: 0.2165 - mean_absolute_error: 0.2165\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1165 - mean_absolute_error: 0.1165\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1078 - mean_absolute_error: 0.1078\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1012 - mean_absolute_error: 0.1012\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0976 - mean_absolute_error: 0.0976\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0983 - mean_absolute_error: 0.0983\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0968 - mean_absolute_error: 0.0968\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0877 - mean_absolute_error: 0.0877\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0861 - mean_absolute_error: 0.0861\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 0s 5ms/step - loss: 0.0879 - mean_absolute_error: 0.0879\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0815 - mean_absolute_error: 0.0815\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0823 - mean_absolute_error: 0.0823\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0764 - mean_absolute_error: 0.0764\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0810 - mean_absolute_error: 0.0810\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0745 - mean_absolute_error: 0.0745\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0721 - mean_absolute_error: 0.0721\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0719 - mean_absolute_error: 0.0719\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0788 - mean_absolute_error: 0.0788\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0773 - mean_absolute_error: 0.0773\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0784 - mean_absolute_error: 0.0784\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0668 - mean_absolute_error: 0.0668\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0761 - mean_absolute_error: 0.0761\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0738 - mean_absolute_error: 0.0738\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0750 - mean_absolute_error: 0.0750\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0684 - mean_absolute_error: 0.0684\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0756 - mean_absolute_error: 0.0756\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0732 - mean_absolute_error: 0.0732\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0662 - mean_absolute_error: 0.0662\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0646 - mean_absolute_error: 0.0646\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0719 - mean_absolute_error: 0.0719\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0642 - mean_absolute_error: 0.0642\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0692 - mean_absolute_error: 0.0692\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0654 - mean_absolute_error: 0.0654\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0731 - mean_absolute_error: 0.0731\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0661 - mean_absolute_error: 0.0661\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0653 - mean_absolute_error: 0.0653\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0619 - mean_absolute_error: 0.0619\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0690 - mean_absolute_error: 0.0690\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0726 - mean_absolute_error: 0.0726\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0676 - mean_absolute_error: 0.0676\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0658 - mean_absolute_error: 0.0658\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0695 - mean_absolute_error: 0.0695\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0642 - mean_absolute_error: 0.0642\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0649 - mean_absolute_error: 0.0649\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.0660 - mean_absolute_error: 0.0660\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0633 - mean_absolute_error: 0.0633\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0657 - mean_absolute_error: 0.0657\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0676 - mean_absolute_error: 0.0676\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0685 - mean_absolute_error: 0.0685\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.0648 - mean_absolute_error: 0.0648\n",
      "10/10 [==============================] - 1s 56ms/step\n",
      "Test score: 0.2442285818445173\n",
      "Accuracy: 0.23896971344947815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelLSTM03_2 = Sequential()\n",
    "modelLSTM03_2.add(LSTM(40, return_sequences=True))\n",
    "modelLSTM03_2.add(LSTM(200, activation='relu'))\n",
    "modelLSTM03_2.add(Dense(40, activation='relu'))\n",
    "modelLSTM03_2.add(Dense(10, activation='relu'))\n",
    "modelLSTM03_2.add(Dense(1, activation='relu'))\n",
    "modelLSTM03_2.compile(optimizer='RMSProp', loss='mae', metrics = ['mae'])\n",
    "\n",
    "\n",
    "modelLSTM03_2.fit(X3_train, Y3_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "LSTMPredicton03_2 = modelLSTM03_2.predict(X3_test)\n",
    "score, mae = modelLSTM03_2.evaluate(X3_test, Y3_test)\n",
    "score = sqrt(mean_squared_error(LSTMPredicton03_2, Y3_test))\n",
    "print('Test score:', score)\n",
    "print('Accuracy:', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con 5 y 10 días tenemos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3388106]\n",
      "[0.45168901]\n",
      "[0.43976593]\n",
      "[0.30081847]\n",
      "[0.66168979]\n",
      "[0.27094981]\n",
      "[0.18912058]\n",
      "[0.14867288]\n",
      "[0.23744154]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1948770909758739"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, j in zip(Y3_test[:-1], LSTMPredicton03_1[:-1]):\n",
    "    print((sqrt((i-j)**2))/i)\n",
    "\n",
    "sqrt(mean_squared_error(Y3_test[:-1], LSTMPredicton03_1[:-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8358209 ],\n",
       "       [0.71641791],\n",
       "       [0.80597015],\n",
       "       [1.29850746],\n",
       "       [0.6119403 ],\n",
       "       [1.71641791],\n",
       "       [2.70149254],\n",
       "       [3.88059701],\n",
       "       [3.11940299],\n",
       "       [2.68656716]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**min_max_scaler4.inverse_transform(Y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.7311673],\n",
       "       [ 5.753436 ],\n",
       "       [ 6.960407 ],\n",
       "       [ 8.781914 ],\n",
       "       [ 9.882771 ],\n",
       "       [12.474416 ],\n",
       "       [16.5651   ],\n",
       "       [22.609966 ],\n",
       "       [28.231226 ],\n",
       "       [33.741417 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**min_max_scaler4.inverse_transform(LSTMPredicton01_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5880318],\n",
       "       [1.8112879],\n",
       "       [2.0703752],\n",
       "       [2.4833922],\n",
       "       [2.6705453],\n",
       "       [3.1286469],\n",
       "       [3.8316662],\n",
       "       [4.798241 ],\n",
       "       [5.505045 ],\n",
       "       [6.056408 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**min_max_scaler4.inverse_transform(LSTMPredicton02_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.489939 ],\n",
       "       [ 2.8638227],\n",
       "       [ 3.271213 ],\n",
       "       [ 3.9076529],\n",
       "       [ 4.1973257],\n",
       "       [ 4.9936695],\n",
       "       [ 6.2027307],\n",
       "       [ 7.8715386],\n",
       "       [ 9.164621 ],\n",
       "       [10.2498   ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**min_max_scaler4.inverse_transform(LSTMPredicton03_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.6649995],\n",
       "       [ 4.343119 ],\n",
       "       [ 5.116244 ],\n",
       "       [ 6.276941 ],\n",
       "       [ 6.8800206],\n",
       "       [ 8.430951 ],\n",
       "       [10.845816 ],\n",
       "       [14.317583 ],\n",
       "       [17.22672  ],\n",
       "       [19.84891  ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**min_max_scaler4.inverse_transform(LSTMPredicton01_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.4011002],\n",
       "       [ 2.7971592],\n",
       "       [ 3.2392294],\n",
       "       [ 3.9392033],\n",
       "       [ 4.29862  ],\n",
       "       [ 5.100867 ],\n",
       "       [ 6.3386188],\n",
       "       [ 8.065435 ],\n",
       "       [ 9.301079 ],\n",
       "       [10.275    ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**min_max_scaler4.inverse_transform(LSTMPredicton02_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.1573858],\n",
       "       [ 3.695786 ],\n",
       "       [ 4.2976484],\n",
       "       [ 5.2409067],\n",
       "       [ 5.719868 ],\n",
       "       [ 6.8666973],\n",
       "       [ 8.635468 ],\n",
       "       [11.124109 ],\n",
       "       [13.08453  ],\n",
       "       [14.749061 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**min_max_scaler4.inverse_transform(LSTMPredicton03_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19921318]\n",
      "[0.3023529]\n",
      "[0.29616755]\n",
      "[0.17704449]\n",
      "[0.50631077]\n",
      "[0.15232036]\n",
      "[0.0795211]\n",
      "[0.04461889]\n",
      "[0.12514726]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1256100474375622"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, j in zip(Y3_test[:-1], LSTMPredicton02_1[:-1]):\n",
    "    print((sqrt((i-j)**2))/i)\n",
    "\n",
    "sqrt(mean_squared_error(Y3_test[:-1], LSTMPredicton02_1[:-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53804782]\n",
      "[0.6791032]\n",
      "[0.67680242]\n",
      "[0.52191789]\n",
      "[0.95595874]\n",
      "[0.50322772]\n",
      "[0.4126264]\n",
      "[0.37047146]\n",
      "[0.4853195]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32952250797949306"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, j in zip(Y3_test[:-1], LSTMPredicton01_1[:-1]):\n",
    "    print((sqrt((i-j)**2))/i)\n",
    "\n",
    "sqrt(mean_squared_error(Y3_test[:-1], LSTMPredicton01_1[:-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of X stack to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking X prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackeo de datos de propagación predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_1_1RO_1 = np.vstack([China_1.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_1.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "Stack_1_1RO_2 = np.vstack([China_2.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_2.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "Stack_1_1RO_3 = np.vstack([China_3.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_3.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "\n",
    "\n",
    "Stack_1_2RO_1 =  Spain_1.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "Stack_1_2RO_2 =  Spain_2.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "Stack_1_2RO_3 =  Spain_3.iloc[:-20, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "\n",
    "\n",
    "Stack_1_3RO_1 = np.vstack((Stack_1_1RO_1,Stack_1_2RO_1))\n",
    "Stack_1_3RO_2 = np.vstack((Stack_1_1RO_2,Stack_1_2RO_2))\n",
    "Stack_1_3RO_3 = np.vstack((Stack_1_1RO_3,Stack_1_2RO_3))\n",
    "\n",
    "\n",
    "Stack_1_4RO_1 =  UK_1.iloc[:-15, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "Stack_1_4RO_2 =  UK_2.iloc[:-15, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "Stack_1_4RO_3 =  UK_3.iloc[:-15, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "\n",
    "Stack_1_5RO_1 = np.vstack((Stack_1_3RO_1,Stack_1_4RO_1))\n",
    "Stack_1_5RO_2 = np.vstack((Stack_1_3RO_2,Stack_1_4RO_2))\n",
    "Stack_1_5RO_3 = np.vstack((Stack_1_3RO_3,Stack_1_4RO_3))\n",
    "\n",
    "\n",
    "\n",
    "Stack_2_1RO_1 = np.vstack([China_1.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_1.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "Stack_2_1RO_2 = np.vstack([China_2.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_2.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "Stack_2_1RO_3 = np.vstack([China_3.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/60, Italy_3.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/61])\n",
    "\n",
    "\n",
    "Stack_2_2RO_1 =  Spain_1.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "Stack_2_2RO_2 =  Spain_2.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "Stack_2_2RO_3 =  Spain_3.iloc[5:-15, [6, 7, 8, 9, 10, 11, 12]].values/47\n",
    "\n",
    "\n",
    "Stack_2_3RO_1 = np.vstack((Stack_2_1RO_1,Stack_2_2RO_1))\n",
    "Stack_2_3RO_2 = np.vstack((Stack_2_1RO_2,Stack_2_2RO_2))\n",
    "Stack_2_3RO_3 = np.vstack((Stack_2_1RO_3,Stack_2_2RO_3))\n",
    "\n",
    "\n",
    "Stack_2_4RO_1 =  UK_1.iloc[5:-10, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "Stack_2_4RO_2 =  UK_2.iloc[5:-10, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "Stack_2_4RO_3 =  UK_3.iloc[5:-10, [6, 7, 8, 9, 10, 11, 12]].values/67\n",
    "\n",
    "Stack_2_5RO_1 = np.vstack((Stack_2_3RO_1,Stack_2_4RO_1))\n",
    "Stack_2_5RO_2 = np.vstack((Stack_2_3RO_2,Stack_2_4RO_2))\n",
    "Stack_2_5RO_3 = np.vstack((Stack_2_3RO_3,Stack_2_4RO_3))\n",
    "\n",
    "\n",
    "Prop1 = np.hstack((Stack_1_5RO_1, Stack_2_5RO_1))\n",
    "Prop2 = np.hstack((Stack_1_5RO_2, Stack_2_5RO_2))\n",
    "Prop3 = np.hstack((Stack_1_5RO_3, Stack_2_5RO_3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackeo de datos de muertes totales - 5/10 días antes - predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_3_1RO_1 = np.vstack([China_1.iloc[10:-10, [4]].values/60, Italy_1.iloc[10:-10, [4]].values/61])\n",
    "Stack_3_1RO_2 = np.vstack([China_2.iloc[10:-10, [4]].values/60, Italy_2.iloc[10:-10, [4]].values/61])\n",
    "Stack_3_1RO_3 = np.vstack([China_3.iloc[10:-10, [4]].values/60, Italy_3.iloc[10:-10, [4]].values/61])\n",
    "\n",
    "\n",
    "Stack_3_2RO_1 =  Spain_1.iloc[10:-10, [4]].values/47\n",
    "Stack_3_2RO_2 =  Spain_2.iloc[10:-10, [4]].values/47\n",
    "Stack_3_2RO_3 =  Spain_3.iloc[10:-10, [4]].values/47\n",
    "\n",
    "\n",
    "Stack_3_3RO_1 = np.vstack((Stack_3_1RO_1,Stack_3_2RO_1))\n",
    "Stack_3_3RO_2 = np.vstack((Stack_3_1RO_2,Stack_3_2RO_2))\n",
    "Stack_3_3RO_3 = np.vstack((Stack_3_1RO_3,Stack_3_2RO_3))\n",
    "\n",
    "\n",
    "Stack_3_4RO_1 =  UK_1.iloc[10:-5, [4]].values/67\n",
    "Stack_3_4RO_2 =  UK_2.iloc[10:-5, [4]].values/67\n",
    "Stack_3_4RO_3 =  UK_3.iloc[10:-5, [4]].values/67\n",
    "\n",
    "Stack_3_5RO_1 = np.vstack((Stack_3_3RO_1,Stack_3_4RO_1))\n",
    "Stack_3_5RO_2 = np.vstack((Stack_3_3RO_2,Stack_3_4RO_2))\n",
    "Stack_3_5RO_3 = np.vstack((Stack_3_3RO_3,Stack_3_4RO_3))\n",
    "\n",
    "\n",
    "\n",
    "Stack_4_1RO_1 = np.vstack([China_1.iloc[15:-5, [4]].values/60, Italy_1.iloc[15:-5, [4]].values/61])\n",
    "Stack_4_1RO_2 = np.vstack([China_2.iloc[15:-5, [4]].values/60, Italy_2.iloc[15:-5, [4]].values/61])\n",
    "Stack_4_1RO_3 = np.vstack([China_3.iloc[15:-5, [4]].values/60, Italy_3.iloc[15:-5, [4]].values/61])\n",
    "\n",
    "\n",
    "Stack_4_2RO_1 =  Spain_1.iloc[15:-5, [4]].values/47\n",
    "Stack_4_2RO_2 =  Spain_2.iloc[15:-5, [4]].values/47\n",
    "Stack_4_2RO_3 =  Spain_3.iloc[15:-5, [4]].values/47\n",
    "\n",
    "\n",
    "Stack_4_3RO_1 = np.vstack((Stack_4_1RO_1,Stack_4_2RO_1))\n",
    "Stack_4_3RO_2 = np.vstack((Stack_4_1RO_2,Stack_4_2RO_2))\n",
    "Stack_4_3RO_3 = np.vstack((Stack_4_1RO_3,Stack_4_2RO_3))\n",
    "\n",
    "\n",
    "Stack_4_4RO_1 =  UK_1.iloc[15:, [4]].values/67\n",
    "Stack_4_4RO_2 =  UK_2.iloc[15:, [4]].values/67\n",
    "Stack_4_4RO_3 =  UK_3.iloc[15:, [4]].values/67\n",
    "\n",
    "Stack_4_5RO_1 = np.vstack((Stack_4_3RO_1,Stack_4_4RO_1))\n",
    "Stack_4_5RO_2 = np.vstack((Stack_4_3RO_2,Stack_4_4RO_2))\n",
    "Stack_4_5RO_3 = np.vstack((Stack_4_3RO_3,Stack_4_4RO_3))\n",
    "\n",
    "\n",
    "Cases1 = np.hstack((Stack_3_5RO_1, Stack_4_5RO_1))\n",
    "Cases2 = np.hstack((Stack_3_5RO_2, Stack_4_5RO_2))\n",
    "Cases3 = np.hstack((Stack_3_5RO_3, Stack_4_5RO_3))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackeo de días predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_5_1RO_1 = np.vstack([China_1.iloc[20:, [5]].values, Italy_1.iloc[20:, [5]].values])\n",
    "Stack_5_1RO_2 = np.vstack([China_2.iloc[20:, [5]].values, Italy_2.iloc[20:, [5]].values])\n",
    "Stack_5_1RO_3 = np.vstack([China_3.iloc[20:, [5]].values, Italy_3.iloc[20:, [5]].values])\n",
    "\n",
    "\n",
    "Stack_5_2RO_1 =  Spain_1.iloc[20:, [5]].values\n",
    "Stack_5_2RO_2 =  Spain_2.iloc[20:, [5]].values\n",
    "Stack_5_2RO_3 =  Spain_3.iloc[20:, [5]].values\n",
    "\n",
    "\n",
    "Stack_5_3RO_1 = np.vstack((Stack_5_1RO_1,Stack_5_2RO_1))\n",
    "Stack_5_3RO_2 = np.vstack((Stack_5_1RO_2,Stack_5_2RO_2))\n",
    "Stack_5_3RO_3 = np.vstack((Stack_5_1RO_3,Stack_5_2RO_3))\n",
    "\n",
    "\n",
    "Stack_5_4RO_1 =  UK_1.iloc[20:, [5]].values\n",
    "Stack_5_4RO_2 =  UK_2.iloc[20:, [5]].values\n",
    "Stack_5_4RO_3 =  UK_3.iloc[20:, [5]].values\n",
    "\n",
    "DaysA1 = np.vstack((Stack_5_3RO_1,Stack_5_4RO_1))\n",
    "DaysA2 = np.vstack((Stack_5_3RO_2,Stack_5_4RO_2))\n",
    "DaysA3 = np.vstack((Stack_5_3RO_3,Stack_5_4RO_3))\n",
    "\n",
    "\n",
    "A = np.vstack([DaysA3[-1]+1, DaysA3[-1]+2, DaysA3[-1]+3, DaysA3[-1]+4, DaysA3[-1]+5])\n",
    "\n",
    "Days1 = np.vstack((DaysA1,A))\n",
    "Days2 = np.vstack((DaysA2,A))\n",
    "Days3 = np.vstack((DaysA3,A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackeo conjunto de la X predic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stackeo1 = np.log10(np.hstack((Prop1, Cases1)))\n",
    "Stackeo2 = np.log10(np.hstack((Prop2, Cases2)))\n",
    "Stackeo2 = np.log10(np.hstack((Prop3, Cases3)))\n",
    "\n",
    "x1 = np.hstack((Stackeo1, Days1))\n",
    "x2 = np.hstack((Stackeo2, Days2))\n",
    "x3 = np.hstack((Stackeo2, Days3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackeo Y (muertes) prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_6_1RO_1 = np.vstack([China_1.iloc[20:, [2]].values/60, Italy_1.iloc[20:, [2]].values/61])\n",
    "Stack_6_1RO_2 = np.vstack([China_2.iloc[20:, [2]].values/60, Italy_2.iloc[20:, [2]].values/61])\n",
    "Stack_6_1RO_3 = np.vstack([China_3.iloc[20:, [2]].values/60, Italy_3.iloc[20:, [2]].values/61])\n",
    "\n",
    "\n",
    "Stack_6_2RO_1 =  Spain_1.iloc[20:, [2]].values/47\n",
    "Stack_6_2RO_2 =  Spain_2.iloc[20:, [2]].values/47\n",
    "Stack_6_2RO_3 =  Spain_3.iloc[20:, [2]].values/47\n",
    "\n",
    "\n",
    "Stack_6_3RO_1 = np.vstack((Stack_6_1RO_1,Stack_6_2RO_1))\n",
    "Stack_6_3RO_2 = np.vstack((Stack_6_1RO_2,Stack_6_2RO_2))\n",
    "Stack_6_3RO_3 = np.vstack((Stack_6_1RO_3,Stack_6_2RO_3))\n",
    "\n",
    "\n",
    "Stack_6_4RO_1 =  UK_1.iloc[20:, [2]].values/67\n",
    "Stack_6_4RO_2 =  UK_2.iloc[20:, [2]].values/67\n",
    "Stack_6_4RO_3 =  UK_3.iloc[20:, [2]].values/67\n",
    "\n",
    "Y1 = np.log10(np.vstack((Stack_6_3RO_1,Stack_6_4RO_1)))\n",
    "Y2 = np.log10(np.vstack((Stack_6_3RO_2,Stack_6_4RO_2)))\n",
    "Y3 = np.log10(np.vstack((Stack_6_3RO_3,Stack_6_4RO_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestra X y nuestra Y podemos realizar un análisis de los componentes principales para reducir el ruido. Nuestra X tiene 15 factores, pero muchos de ellos son redundantes, podemos probar a reducirlo a entre 10 y 8 por ejemplo. También normalizamos esos factores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing training and test + Feature selection 1 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1[x1 == -inf] = 0\n",
    "x2[x2 == -inf] = 0\n",
    "x3[x3 == -inf] = 0\n",
    "\n",
    "Y1[Y1 == -inf] = 0\n",
    "Y2[Y2 == -inf] = 0\n",
    "Y3[Y3 == -inf] = 0\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X1 = min_max_scaler1.fit_transform(x1)\n",
    "\n",
    "X2 = min_max_scaler2.fit_transform(x2)\n",
    "\n",
    "X3 = min_max_scaler3.fit_transform(x3)\n",
    "\n",
    "Y1 = min_max_scaler4.fit_transform(Y1)\n",
    "\n",
    "Y2 = min_max_scaler5.fit_transform(Y2)\n",
    "\n",
    "Y3 = min_max_scaler6.fit_transform(Y3)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "X1 = featureselector1.transform(X1)\n",
    "\n",
    "X2 = featureselector2.transform(X2)\n",
    "\n",
    "X3 = featureselector3.transform(X3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X1_train = X1[:-5].reshape(len(X1)-5, 1, X1.shape[1])\n",
    "X2_train = X2[:-5].reshape(len(X2)-5, 1, X2.shape[1])\n",
    "X3_train = X3[:-5].reshape(len(X3)-5, 1, X3.shape[1])\n",
    "\n",
    "X1_test = X1[-5:].reshape(5, 1, X1.shape[1])\n",
    "X2_test = X2[-5:].reshape(5, 1, X2.shape[1])\n",
    "X3_test = X3[-5:].reshape(5, 1, X3.shape[1])\n",
    "\n",
    "\n",
    "Y1_train = Y1[:]\n",
    "Y2_train = Y2[:]\n",
    "Y3_train = Y3[:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "106/106 [==============================] - 0s 138us/step - loss: 0.0559 - mean_absolute_error: 0.0559\n",
      "Epoch 1/1\n",
      "106/106 [==============================] - 0s 133us/step - loss: 0.0706 - mean_absolute_error: 0.0706\n",
      "Epoch 1/1\n",
      "106/106 [==============================] - 0s 149us/step - loss: 0.0685 - mean_absolute_error: 0.0685\n",
      "Epoch 1/1\n",
      "106/106 [==============================] - 0s 132us/step - loss: 0.0536 - mean_absolute_error: 0.0536\n",
      "Epoch 1/1\n",
      "106/106 [==============================] - 0s 128us/step - loss: 0.0570 - mean_absolute_error: 0.0570\n",
      "Epoch 1/1\n",
      "106/106 [==============================] - 0s 129us/step - loss: 0.0535 - mean_absolute_error: 0.0535\n"
     ]
    }
   ],
   "source": [
    "modelLSTM01_1.fit(X1_train, Y1_train)\n",
    "modelLSTM02_1.fit(X2_train, Y2_train)\n",
    "modelLSTM03_1.fit(X3_train, Y3_train)\n",
    "\n",
    "\n",
    "NormalizedPrediction1_1 = 10**(min_max_scaler4.inverse_transform(modelLSTM01_1.predict(X1_test)))\n",
    "NormalizedPrediction2_1 = 10**(min_max_scaler5.inverse_transform(modelLSTM02_1.predict(X2_test)))\n",
    "NormalizedPrediction3_1 = 10**(min_max_scaler6.inverse_transform(modelLSTM03_1.predict(X3_test)))\n",
    "\n",
    "\n",
    "\n",
    "modelLSTM01_2.fit(X1_train, Y1_train)\n",
    "modelLSTM02_2.fit(X2_train, Y2_train)\n",
    "modelLSTM03_2.fit(X3_train, Y3_train)\n",
    "\n",
    "\n",
    "NormalizedPrediction1_2 = 10**(min_max_scaler4.inverse_transform(modelLSTM01_2.predict(X1_test)))\n",
    "NormalizedPrediction2_2 = 10**(min_max_scaler5.inverse_transform(modelLSTM02_2.predict(X2_test)))\n",
    "NormalizedPrediction3_2 = 10**(min_max_scaler6.inverse_transform(modelLSTM03_2.predict(X3_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1600, plot_height=800, title='Muertes por día - Predicción con LSTM - UK', x_axis_label='Day', x_axis_type='datetime', y_axis_label='Personas')\n",
    "p.line(pd.to_datetime(UK[UK['new_deaths']>10]['date']), UK[UK['new_deaths']>10]['new_deaths'].values, color='blue', alpha=0.75, line_width=3, legend='UK ')\n",
    "p.line(np.hstack([pd.to_datetime(UK[UK['new_deaths']>10]['date'])[-1:], dates]), np.ravel(np.vstack([UK[UK['new_deaths']>10]['new_deaths'].values[-1:], NormalizedPrediction1_1])), color='purple', alpha=0.5, line_width=5, legend='UK evolution RO 1 Adamax')\n",
    "p.line(np.hstack([pd.to_datetime(UK[UK['new_deaths']>10]['date'])[-1:], dates]), np.ravel(np.vstack([UK[UK['new_deaths']>10]['new_deaths'].values[-1:], NormalizedPrediction1_2])), color='purple', line_dash='dashed', alpha=0.5, line_width=5, legend='UK evolution RO 1 RSMprop')\n",
    "p.line(np.hstack([pd.to_datetime(UK[UK['new_deaths']>10]['date'])[-1:], dates]), np.ravel(np.vstack([UK[UK['new_deaths']>10]['new_deaths'].values[-1:], NormalizedPrediction3_2])), color='orange', alpha=0.5, line_width=5, legend='UK evolution RO 3 RSMprop')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1600, plot_height=800, title='Muertes por día - Predicción con LSTM - UK', x_axis_label='Day', x_axis_type='datetime', y_axis_label='Personas')\n",
    "p.line(np.arange(0, len(X1_train)), 10**np.ravel(modelLSTM01_1.predict(X1_train)), color='purple', alpha=0.75, line_width=3, legend='training RO1 Adamax')\n",
    "p.line(np.arange(0, len(X1_train)), 10**np.ravel(modelLSTM01_2.predict(X1_train)), color='purple', line_dash='dashed',alpha=0.75, line_width=3, legend='training RO1 RSMprop')\n",
    "p.line(np.arange(0, len(X1_train)), 10**np.ravel(modelLSTM02_1.predict(X2_train)), color='red', alpha=0.75, line_width=3, legend='training RO2 Adamax')\n",
    "p.line(np.arange(0, len(X1_train)), 10**np.ravel(modelLSTM02_2.predict(X2_train)), color='red', line_dash='dashed',alpha=0.75, line_width=3, legend='training RO2 RSMprop')\n",
    "p.line(np.arange(0, len(X1_train)), 10**np.ravel(modelLSTM03_1.predict(X3_train)), color='orange', alpha=0.75, line_width=3, legend='training RO3 Adamax')\n",
    "p.line(np.arange(0, len(X1_train)), 10**np.ravel(modelLSTM03_2.predict(X3_train)), color='orange', line_dash='dashed', alpha=0.75, line_width=3, legend='training RO3 RSMprop')\n",
    "p.line(np.arange(0, len(X1_train)), 10**np.ravel(Y1_train), color='black', alpha=0.75, line_width=3, legend='data ')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [10**np.ravel(modelLSTM01_1.predict(X1_train)), 10**np.ravel(modelLSTM01_2.predict(X1_train)), 10**np.ravel(modelLSTM02_2.predict(X2_train))]\n",
    "train = sum(train)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1600, plot_height=800, title='Muertes por día - Predicción con LSTM - training', x_axis_label='Day', y_axis_label='Personas')\n",
    "p.line(np.arange(0, len(X1_train)), train, color='orange', alpha=0.75, line_width=3, legend='prediction over training set')\n",
    "p.line(np.arange(0, len(X1_train)), 10**np.ravel(Y1_train), color='black', alpha=0.75, line_width=3, legend='data ')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [10**np.ravel(modelLSTM01_1.predict(X1_test)), 10**np.ravel(modelLSTM01_2.predict(X1_test)), 10**np.ravel(modelLSTM02_2.predict(X2_test))]\n",
    "test = sum(test)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.745355,  9.719025, 10.912306, 11.852003, 12.666037],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**np.ravel(modelLSTM01_1.predict(X1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UK[UK['new_deaths']>10]['new_deaths'].values[-1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "d = timedelta(days=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [pd.to_datetime(UK['date'][-1:].values), pd.to_datetime(UK['date'][-1:].values)+timedelta(days=1), pd.to_datetime(UK['date'][-1:].values)+timedelta(days=2), pd.to_datetime(UK['date'][-1:].values)+timedelta(days=3), pd.to_datetime(UK['date'][-1:].values)+timedelta(days=4), pd.to_datetime(UK['date'][-1:].values)+timedelta(days=5)]                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1600, plot_height=800, title='Muertes por día - Predicción con LSTM - UK', x_axis_label='Day', x_axis_type='datetime', y_axis_label='Personas')\n",
    "p.line(pd.to_datetime(UK[UK['new_deaths']>5]['date']), UK[UK['new_deaths']>5]['new_deaths'].values, color='black', alpha=0.75, line_width=3, legend='UK ')\n",
    "p.line(np.ravel(dates), np.hstack([np.ravel(UK['new_deaths'][-1:]), np.ravel(NormalizedPrediction3_2*67)]), color='orange', alpha=0.5, line_width=5, legend='UK predicted evolution')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1600, plot_height=800, title='Muertes por Millón por día - datos de entrenamiento', x_axis_label='Day', x_axis_type='datetime', y_axis_label='Personas')\n",
    "p.line(np.arange(0, len(X1_train)), np.ravel(10**(min_max_scaler6.inverse_transform(modelLSTM03_2.predict(X3_train)))), color='orange', line_dash='dashed', alpha=0.75, line_width=3, legend='training RO3 RSMprop')\n",
    "p.line(np.arange(0, len(X1_train)), np.ravel(10**(min_max_scaler4.inverse_transform(Y1))), color='black', alpha=0.75, line_width=3, legend='data ')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficas comparativas de muertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpainCases = Data[Data['location']=='Spain'].drop('location', axis=1)\n",
    "ItalyCases = Data[Data['location']=='Italy'].drop('location', axis=1)\n",
    "ChinaCases = Data[Data['location']=='China'].drop('location', axis=1)\n",
    "GermanyCases = Data[Data['location']=='Germany'].drop('location', axis=1)\n",
    "FranceCases = Data[Data['location']=='France'].drop('location', axis=1)\n",
    "UKCases = Data[Data['location']=='United Kingdom'].drop('location', axis=1)\n",
    "KoreaCases = Data[Data['location']=='South Korea'].drop('location', axis=1)\n",
    "PortugalCases = Data[Data['location']=='Portugal'].drop('location', axis=1)\n",
    "NetherlandsCases = Data[Data['location']=='Netherlands'].drop('location', axis=1)\n",
    "USACases = Data[Data['location']=='United States'].drop('location', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpainCasos = SpainCases[SpainCases['total_cases']>50]['new_cases']\n",
    "ItalyCasos = ItalyCases[ItalyCases['total_cases']>50]['new_cases']\n",
    "GermanyCasos = GermanyCases[GermanyCases['total_cases']>50]['new_cases']\n",
    "FranceCasos = FranceCases[FranceCases['total_cases']>50]['new_cases']\n",
    "UKCasos = UKCases[UKCases['total_cases']>50]['new_cases']\n",
    "PortugalCasos = PortugalCases[PortugalCases['total_cases']>50]['new_cases']\n",
    "NetherlandsCasos = NetherlandsCases[NetherlandsCases['total_cases']>50]['new_cases']\n",
    "USACasos = USACases[USACases['total_cases']>50]['new_cases']\n",
    "ChinaCasos = ChinaCases[ChinaCases['total_cases']>50]['new_cases']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpainDeaths = SpainCases[SpainCases['total_cases']>50]['new_deaths']\n",
    "ItalyDeaths = ItalyCases[ItalyCases['total_cases']>50]['new_deaths']\n",
    "GermanyDeaths = GermanyCases[GermanyCases['total_cases']>50]['new_deaths']\n",
    "FranceDeaths = FranceCases[FranceCases['total_cases']>50]['new_deaths']\n",
    "UKDeaths = UKCases[UKCases['total_cases']>50]['new_deaths']\n",
    "PortugalDeaths = PortugalCases[PortugalCases['total_cases']>50]['new_deaths']\n",
    "NetherlandsDeaths = NetherlandsCases[NetherlandsCases['total_cases']>50]['new_deaths']\n",
    "USADeaths = USACases[USACases['total_cases']>50]['new_deaths']\n",
    "ChinaDeaths = ChinaCases[ChinaCases['total_cases']>50]['new_deaths']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpainTotalCasos = SpainCases[SpainCases['total_cases']>50]['total_cases']\n",
    "ItalyTotalCasos = ItalyCases[ItalyCases['total_cases']>50]['total_cases']\n",
    "GermanyTotalCasos = GermanyCases[GermanyCases['total_cases']>50]['total_cases']\n",
    "FranceTotalCasos = FranceCases[FranceCases['total_cases']>50]['total_cases']\n",
    "UKTotalCasos = UKCases[UKCases['total_cases']>50]['total_cases']\n",
    "PortugalTotalCasos = PortugalCases[PortugalCases['total_cases']>50]['total_cases']\n",
    "NetherlandsTotalCasos = NetherlandsCases[NetherlandsCases['total_cases']>50]['total_cases']\n",
    "USATotalCasos = USACases[USACases['total_cases']>50]['total_cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpainTotalDeaths = SpainCases[SpainCases['total_cases']>50]['total_deaths']\n",
    "ItalyTotalDeaths = ItalyCases[ItalyCases['total_cases']>50]['total_deaths']\n",
    "GermanyTotalDeaths = GermanyCases[GermanyCases['total_cases']>50]['total_deaths']\n",
    "FranceTotalDeaths = FranceCases[FranceCases['total_cases']>50]['total_deaths']\n",
    "UKTotalDeaths = UKCases[UKCases['total_cases']>50]['total_deaths']\n",
    "PortugalTotalDeaths = PortugalCases[PortugalCases['total_cases']>50]['total_deaths']\n",
    "NetherlandsTotalDeaths = NetherlandsCases[NetherlandsCases['total_cases']>50]['total_deaths']\n",
    "USATotalDeaths = USACases[USACases['total_cases']>50]['total_deaths']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1000, plot_height=800, title='Casos por día per M tomando como día 0 el momento en que hay más de 50 casos registrados', x_axis_label='Day', y_axis_label='Personas')\n",
    "p.line(np.arange(0,SpainCasos.shape[0]), SpainCasos/47, color='red', alpha=0.75, line_width=3, legend='Spain ')\n",
    "p.line(np.arange(0,ItalyCasos.shape[0]), ItalyCasos/61, color='green', alpha=0.75, line_width=3, legend='Italy ')\n",
    "p.line(np.arange(0,FranceCasos.shape[0]), FranceCasos/67, color='blue', alpha=0.75, line_width=3, legend='France ')\n",
    "p.line(np.arange(0,GermanyCasos.shape[0]), GermanyCasos/83, color='black', alpha=0.75, line_width=3, legend='Germany ')\n",
    "p.line(np.arange(0,NetherlandsCasos.shape[0]), NetherlandsCasos/17, color='orange', alpha=0.75, line_width=3, legend='Netherlands ')\n",
    "p.line(np.arange(0,PortugalCasos.shape[0]), PortugalCasos/10, color='purple', alpha=0.75, line_width=3, legend='Portugal ')\n",
    "p.line(np.arange(0,UKCasos.shape[0]), UKCasos/67, color='Aqua', alpha=0.75, line_width=3, legend='UK')\n",
    "p.line(np.arange(0,USACasos.shape[0]), USACasos/327, color='DarkCyan', alpha=0.75, line_width=3, legend='USA')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1200, plot_height=800, title='Muertes por día per Million', x_axis_label='Day', y_axis_label='Personas')\n",
    "p.line(np.arange(0,SpainDeaths.shape[0]), SpainDeaths/47, color='red', alpha=0.75, line_width=3, legend='Spain ')\n",
    "p.line(np.arange(0,ItalyDeaths.shape[0]), ItalyDeaths/61, color='green', alpha=0.75, line_width=3, legend='Italy ')\n",
    "p.line(np.arange(0,FranceDeaths.shape[0]), FranceDeaths/67, color='blue', alpha=0.75, line_width=3, legend='France ')\n",
    "p.line(np.arange(0,GermanyDeaths.shape[0]), GermanyDeaths/83, color='black', alpha=0.75, line_width=3, legend='Germany ')\n",
    "p.line(np.arange(0,NetherlandsDeaths.shape[0]), NetherlandsDeaths/17, color='orange', alpha=0.75, line_width=3, legend='Netherlands ')\n",
    "p.line(np.arange(0,PortugalDeaths.shape[0]), PortugalDeaths/10, color='purple', alpha=0.75, line_width=3, legend='Portugal ')\n",
    "p.line(np.arange(0,UKDeaths.shape[0]), UKDeaths/67, color='Aqua', alpha=0.75, line_width=3, legend='UK')\n",
    "p.line(np.arange(0,USADeaths.shape[0]), USADeaths/327, color='DarkCyan', alpha=0.75, line_width=3, legend='USA')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1000, plot_height=800, title='TotalCasos acumulados per M tomando como día 0 el momento en que hay más de 50 TotalCasos registrados', x_axis_label='Day', y_axis_label='Personas')\n",
    "p.line(np.arange(0,SpainTotalCasos.shape[0]), SpainTotalCasos/47, color='red', alpha=0.75, line_width=3, legend='Spain ')\n",
    "p.line(np.arange(0,ItalyTotalCasos.shape[0]), ItalyTotalCasos/61, color='green', alpha=0.75, line_width=3, legend='Italy ')\n",
    "p.line(np.arange(0,FranceTotalCasos.shape[0]), FranceTotalCasos/67, color='blue', alpha=0.75, line_width=3, legend='France ')\n",
    "p.line(np.arange(0,GermanyTotalCasos.shape[0]), GermanyTotalCasos/83, color='black', alpha=0.75, line_width=3, legend='Germany ')\n",
    "p.line(np.arange(0,NetherlandsTotalCasos.shape[0]), NetherlandsTotalCasos/17, color='orange', alpha=0.75, line_width=3, legend='Netherlands ')\n",
    "p.line(np.arange(0,PortugalTotalCasos.shape[0]), PortugalTotalCasos/10, color='purple', alpha=0.75, line_width=3, legend='Portugal ')\n",
    "p.line(np.arange(0,UKTotalCasos.shape[0]), UKTotalCasos/67, color='Aqua', alpha=0.75, line_width=3, legend='UK')\n",
    "p.line(np.arange(0,USATotalCasos.shape[0]), USATotalCasos/327, color='DarkCyan', alpha=0.75, line_width=3, legend='USA')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1000, plot_height=800, title='Muertes perM acumuladas tomando como día 0 el momento en que hay más de 50 TotalDeaths registrados', x_axis_label='Day', y_axis_label='Personas')\n",
    "p.line(np.arange(0,SpainTotalDeaths.shape[0]), SpainTotalDeaths/47, color='red', alpha=0.75, line_width=3, legend='Spain ')\n",
    "p.line(np.arange(0,ItalyTotalDeaths.shape[0]), ItalyTotalDeaths/61, color='green', alpha=0.75, line_width=3, legend='Italy ')\n",
    "p.line(np.arange(0,FranceTotalDeaths.shape[0]), FranceTotalDeaths/67, color='blue', alpha=0.75, line_width=3, legend='France ')\n",
    "p.line(np.arange(0,GermanyTotalDeaths.shape[0]), GermanyTotalDeaths/83, color='black', alpha=0.75, line_width=3, legend='Germany ')\n",
    "p.line(np.arange(0,NetherlandsTotalDeaths.shape[0]), NetherlandsTotalDeaths/17, color='orange', alpha=0.75, line_width=3, legend='Netherlands ')\n",
    "p.line(np.arange(0,PortugalTotalDeaths.shape[0]), PortugalTotalDeaths/10, color='purple', alpha=0.75, line_width=3, legend='Portugal ')\n",
    "p.line(np.arange(0,UKTotalDeaths.shape[0]), UKTotalDeaths/67, color='Aqua', alpha=0.75, line_width=3, legend='UK')\n",
    "p.line(np.arange(0,USATotalDeaths.shape[0]), USATotalDeaths/327, color='DarkCyan', alpha=0.75, line_width=3, legend='USA')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(plot_width=1200, plot_height=800, title='Muertes y casos diarios', x_axis_label='Day', y_axis_label='Personas por millón')\n",
    "p.line(np.arange(0,SpainDeaths.shape[0]), SpainDeaths/47, color='red', alpha=0.75, line_dash='dashed', line_width=3, legend='España, muertes ')\n",
    "p.line(np.arange(0,SpainCasos.shape[0]), SpainCasos/470, color='red', alpha=0.75, line_width=3, legend='España, muertes')\n",
    "p.line(np.arange(0,ItalyDeaths.shape[0]), ItalyDeaths/61, color='green', alpha=0.75, line_dash='dashed', line_width=3, legend='Italia, muertes')\n",
    "p.line(np.arange(0,ItalyCasos.shape[0]), ItalyCasos/610, color='green', alpha=0.75, line_width=3, legend='Italia, muertes')\n",
    "p.line(np.arange(0,FranceDeaths.shape[0]), FranceDeaths/67, color='blue', alpha=0.75, line_dash='dashed', line_width=3, legend='Francia, muertes')\n",
    "p.line(np.arange(0,FranceCasos.shape[0]), FranceCasos/670, color='blue', alpha=0.75, line_width=3, legend='Francia, muertes')\n",
    "p.line(np.arange(0,GermanyDeaths.shape[0]), GermanyDeaths/61, color='black', alpha=0.75, line_dash='dashed', line_width=3, legend='Alemania, muertes')\n",
    "p.line(np.arange(0,GermanyCasos.shape[0]), GermanyCasos/610, color='black', alpha=0.75, line_width=3, legend='Alemania, muertes')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ItalyNorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-daa82e641ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Muertes y casos diarios en Italia, aplicando un retardo - Normalizado'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_axis_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_axis_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Personas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mItalyNorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormItaly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Normalised new_cases'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_dash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Italia, Casos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mItalyNorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormItaly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Normalised new_deaths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Italia, muertes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ItalyNorm' is not defined"
     ]
    }
   ],
   "source": [
    "p = figure(plot_width=1200, plot_height=800, title='Muertes y casos diarios en Italia, aplicando un retardo - Normalizado', x_axis_label='Day', y_axis_label='Personas')\n",
    "p.line(np.arange(0,ItalyNorm.shape[0]-5), NormItaly['Normalised new_cases'][:-5], color='blue', alpha=0.75, line_dash='dashed', line_width=3, legend='Italia, Casos')\n",
    "p.line(np.arange(0,ItalyNorm.shape[0]-5), NormItaly['Normalised new_deaths'][5:], color='blue', alpha=0.75, line_width=3, legend='Italia, muertes')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>total_deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7915</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7916</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7917</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7918</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7920</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7921</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7923</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7924</th>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7925</th>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7927</th>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7928</th>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7929</th>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>2020-01-18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7937</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7938</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7939</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7940</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7941</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7942</th>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7943</th>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7978</th>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7979</th>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7980</th>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7981</th>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>206</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7984</th>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>321</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7985</th>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7986</th>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>456</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>134</td>\n",
       "      <td>4</td>\n",
       "      <td>590</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>707</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>433</td>\n",
       "      <td>11</td>\n",
       "      <td>1140</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>251</td>\n",
       "      <td>14</td>\n",
       "      <td>1391</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>152</td>\n",
       "      <td>20</td>\n",
       "      <td>1543</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>407</td>\n",
       "      <td>5</td>\n",
       "      <td>1950</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>680</td>\n",
       "      <td>43</td>\n",
       "      <td>2630</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>647</td>\n",
       "      <td>41</td>\n",
       "      <td>3277</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>706</td>\n",
       "      <td>33</td>\n",
       "      <td>3983</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>1035</td>\n",
       "      <td>56</td>\n",
       "      <td>5018</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>665</td>\n",
       "      <td>48</td>\n",
       "      <td>5683</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>967</td>\n",
       "      <td>54</td>\n",
       "      <td>6650</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>1427</td>\n",
       "      <td>87</td>\n",
       "      <td>8077</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>1452</td>\n",
       "      <td>41</td>\n",
       "      <td>9529</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8001</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>2129</td>\n",
       "      <td>115</td>\n",
       "      <td>11658</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8002</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>2885</td>\n",
       "      <td>181</td>\n",
       "      <td>14543</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8003</th>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>2546</td>\n",
       "      <td>260</td>\n",
       "      <td>17089</td>\n",
       "      <td>1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8004</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>2433</td>\n",
       "      <td>209</td>\n",
       "      <td>19522</td>\n",
       "      <td>1228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8005</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2619</td>\n",
       "      <td>180</td>\n",
       "      <td>22141</td>\n",
       "      <td>1408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8006</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>3009</td>\n",
       "      <td>381</td>\n",
       "      <td>25150</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8007</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>4324</td>\n",
       "      <td>743</td>\n",
       "      <td>29474</td>\n",
       "      <td>2532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  new_cases  new_deaths  total_cases  total_deaths\n",
       "7914  2019-12-31          0           0            0             0\n",
       "7915  2020-01-01          0           0            0             0\n",
       "7916  2020-01-02          0           0            0             0\n",
       "7917  2020-01-03          0           0            0             0\n",
       "7918  2020-01-04          0           0            0             0\n",
       "7919  2020-01-05          0           0            0             0\n",
       "7920  2020-01-06          0           0            0             0\n",
       "7921  2020-01-07          0           0            0             0\n",
       "7922  2020-01-08          0           0            0             0\n",
       "7923  2020-01-09          0           0            0             0\n",
       "7924  2020-01-10          0           0            0             0\n",
       "7925  2020-01-11          0           0            0             0\n",
       "7926  2020-01-12          0           0            0             0\n",
       "7927  2020-01-13          0           0            0             0\n",
       "7928  2020-01-14          0           0            0             0\n",
       "7929  2020-01-15          0           0            0             0\n",
       "7930  2020-01-16          0           0            0             0\n",
       "7931  2020-01-17          0           0            0             0\n",
       "7932  2020-01-18          0           0            0             0\n",
       "7933  2020-01-19          0           0            0             0\n",
       "7934  2020-01-20          0           0            0             0\n",
       "7935  2020-01-21          0           0            0             0\n",
       "7936  2020-01-22          0           0            0             0\n",
       "7937  2020-01-23          0           0            0             0\n",
       "7938  2020-01-24          0           0            0             0\n",
       "7939  2020-01-25          0           0            0             0\n",
       "7940  2020-01-26          0           0            0             0\n",
       "7941  2020-01-27          0           0            0             0\n",
       "7942  2020-01-28          0           0            0             0\n",
       "7943  2020-01-29          0           0            0             0\n",
       "...          ...        ...         ...          ...           ...\n",
       "7978  2020-03-04         11           0           51             0\n",
       "7979  2020-03-05         34           0           85             0\n",
       "7980  2020-03-06         30           1          115             1\n",
       "7981  2020-03-07         48           0          163             1\n",
       "7982  2020-03-08         43           1          206             2\n",
       "7983  2020-03-09         67           1          273             3\n",
       "7984  2020-03-10         48           2          321             5\n",
       "7985  2020-03-11         52           1          373             6\n",
       "7986  2020-03-12         83           0          456             6\n",
       "7987  2020-03-13        134           4          590            10\n",
       "7988  2020-03-14        117           0          707            10\n",
       "7989  2020-03-15        433          11         1140            21\n",
       "7990  2020-03-16        251          14         1391            35\n",
       "7991  2020-03-17        152          20         1543            55\n",
       "7992  2020-03-18        407           5         1950            60\n",
       "7993  2020-03-19        680          43         2630           103\n",
       "7994  2020-03-20        647          41         3277           144\n",
       "7995  2020-03-21        706          33         3983           177\n",
       "7996  2020-03-22       1035          56         5018           233\n",
       "7997  2020-03-23        665          48         5683           281\n",
       "7998  2020-03-24        967          54         6650           335\n",
       "7999  2020-03-25       1427          87         8077           422\n",
       "8000  2020-03-26       1452          41         9529           463\n",
       "8001  2020-03-27       2129         115        11658           578\n",
       "8002  2020-03-28       2885         181        14543           759\n",
       "8003  2020-03-29       2546         260        17089          1019\n",
       "8004  2020-03-30       2433         209        19522          1228\n",
       "8005  2020-03-31       2619         180        22141          1408\n",
       "8006  2020-04-01       3009         381        25150          1789\n",
       "8007  2020-04-02       4324         743        29474          2532\n",
       "\n",
       "[94 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UKCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
